{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPExdDwXiBVb+FIoPOjNUV"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "": ""
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Adaptive RAG System with Metacognitive Learning"
      ],
      "metadata": {
        "id": "0wUs8RBlEAvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution function with comprehensive visualizations\n",
        "def main():\n",
        "    \"\"\"Main execution function for Colab with advanced visualizations\"\"\"\n",
        "    print(\"ü§ñ Self-Learning RAG Agent with Z-Reasoning\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"This implementation demonstrates a comprehensive RAG system with:\")\n",
        "    print(\"‚Ä¢ Multi-agent architecture (Planner, Retriever, Reasoner, Tool, Critic)\")\n",
        "    print(\"‚Ä¢ Self-assessment and learning capabilities\")\n",
        "    print(\"‚Ä¢ Z-reasoning for continuous improvement\")\n",
        "    print(\"‚Ä¢ Advanced analytics with 15+ visualization types\")\n",
        "    print(\"‚Ä¢ Machine learning performance analysis\")\n",
        "    print(\"‚Ä¢ Comprehensive reporting system\")\n",
        "\n",
        "    # Run demonstrations\n",
        "    try:\n",
        "        # System architecture analysis\n",
        "        analyze_system_architecture()\n",
        "\n",
        "        # Learning mechanism analysis\n",
        "        create_learning_mechanism_analysis()\n",
        "\n",
        "        # Main demonstration with extended queries for better analytics\n",
        "        agent = SelfLearningRAGAgent()\n",
        "\n",
        "        # Extended demo queries for richer analytics\n",
        "        extended_demo_queries = [\n",
        "            \"What is artificial intelligence and how does it work?\",\n",
        "            \"How does machine learning differ from deep learning?\",\n",
        "            \"Explain the concept of reinforcement learning with examples\",\n",
        "            \"Compare natural language processing and computer vision techniques\",\n",
        "            \"What are the advantages and disadvantages of large language models?\",\n",
        "            \"How does RAG improve AI system performance and accuracy?\",\n",
        "            \"What are the ethical implications of AI development?\",\n",
        "            \"Describe the relationship between AI, ML, and deep learning\",\n",
        "            \"How do neural networks learn and adapt over time?\",\n",
        "            \"What is the role of data in machine learning systems?\",\n",
        "            \"Explain the concept of transfer learning in AI\",\n",
        "            \"How do attention mechanisms work in transformers?\",\n",
        "            \"What are the challenges in developing AGI?\",\n",
        "            \"Compare supervised and unsupervised learning approaches\",\n",
        "            \"How does federated learning preserve privacy?\",\n",
        "            \"What is the importance of explainable AI?\",\n",
        "            \"How do generative models create new content?\",\n",
        "            \"What are the applications of computer vision?\",\n",
        "            \"Explain the concept of few-shot learning\",\n",
        "            \"How does reinforcement learning from human feedback work?\"\n",
        "        ]\n",
        "\n",
        "        print(f\"\\nüìù Processing {len(extended_demo_queries)} Demo Queries for Rich Analytics...\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for i, query in enumerate(extended_demo_queries, 1):\n",
        "            print(f\"\\nQuery {i:2d}: {query[:50]}{'...' if len(query) > 50 else ''}\")\n",
        "\n",
        "            result = agent.process_query(query)\n",
        "            results.append(result)\n",
        "\n",
        "            # Show progress indicators\n",
        "            if i % 5 == 0:\n",
        "                current_avg = np.mean([r['critique']['overall_score'] for r in results[-5:]])\n",
        "                print(f\"   ‚Üí Last 5 queries average score: {current_avg:.3f}\")\n",
        "\n",
        "        print(f\"\\n‚úÖ All queries processed successfully!\")\n",
        "\n",
        "        # Get comprehensive analytics\n",
        "        analytics = agent.get_performance_analytics()\n",
        "\n",
        "        # Display summary\n",
        "        print(f\"\\nüìä PERFORMANCE SUMMARY:\")\n",
        "        print(f\"   Total Queries: {len(results)}\")\n",
        "        print(f\"   Average Score: {analytics['summary_stats']['avg_overall_score']:.3f}\")\n",
        "        print(f\"   Learning Trend: {analytics['trends']['score_improvement']:+.3f}\")\n",
        "        print(f\"   Error Rate: {analytics['summary_stats']['hallucination_rate']:.1%}\")\n",
        "\n",
        "        # Generate all visualizations\n",
        "        print(f\"\\nüé® Generating Comprehensive Visualizations...\")\n",
        "        print(\"   This may take a few moments...\")\n",
        "\n",
        "        # Core visualizations (built into the agent)\n",
        "        agent.visualize_performance()\n",
        "\n",
        "        # Advanced analytics suite\n",
        "        create_advanced_analytics_suite(agent)\n",
        "\n",
        "        # Generate comprehensive report\n",
        "        report = create_comprehensive_report(agent, results, analytics)\n",
        "\n",
        "        print(f\"\\nüéâ DEMONSTRATION COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"Generated visualization types:\")\n",
        "        print(\"üìä Interactive Dashboard (6 charts)\")\n",
        "        print(\"üìà Statistical Analysis (6 charts)\")\n",
        "        print(\"‚è±Ô∏è  Temporal Analysis (4 charts)\")\n",
        "        print(\"‚öñÔ∏è  Comparative Analysis (4 charts)\")\n",
        "        print(\"üß† Learning Analytics (6 charts)\")\n",
        "        print(\"üîç Error Analysis (6 charts)\")\n",
        "        print(\"üåê 3D Analysis (4 charts)\")\n",
        "        print(\"üï∏Ô∏è  Network Analysis\")\n",
        "        print(\"ü§ñ ML Clustering Analysis\")\n",
        "        print(\"üìà Forecasting Analysis\")\n",
        "        print(\"üö® Anomaly Detection\")\n",
        "        print(\"üîó Causal Analysis\")\n",
        "        print(\"üìã Comprehensive Report\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Total: 40+ different visualizations and analyses!\")\n",
        "\n",
        "        return agent, results, analytics, report\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during execution: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None, None, None\n",
        "\n",
        "def create_visualization_showcase():\n",
        "    \"\"\"Create a showcase of all visualization types\"\"\"\n",
        "    print(\"\\nüé® VISUALIZATION SHOWCASE\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"This demonstration includes:\")\n",
        "\n",
        "    viz_types = {\n",
        "        \"Interactive Plotly Charts\": [\n",
        "            \"Multi-metric timeline with dual axes\",\n",
        "            \"Radar charts for component performance\",\n",
        "            \"Box plots for distribution analysis\",\n",
        "            \"Learning velocity tracking\",\n",
        "            \"Confidence vs accuracy scatter plots\",\n",
        "            \"Strategy effectiveness over time\"\n",
        "        ],\n",
        "        \"Statistical Analysis\": [\n",
        "            \"Distribution analysis with KDE\",\n",
        "            \"Correlation matrices with annotations\",\n",
        "            \"Performance quartile analysis\",\n",
        "            \"Confidence interval bands\",\n",
        "            \"Hexbin plots for complexity analysis\",\n",
        "            \"Outlier detection visualizations\"\n",
        "        ],\n",
        "        \"Temporal Analysis\": [\n",
        "            \"STL decomposition simulation\",\n",
        "            \"Seasonal pattern detection\",\n",
        "            \"Multiple moving averages comparison\",\n",
        "            \"Volatility analysis with rolling statistics\"\n",
        "        ],\n",
        "        \"Learning Analytics\": [\n",
        "            \"Learning curves with confidence bands\",\n",
        "            \"Improvement rate tracking\",\n",
        "            \"Strategy evolution heatmaps\",\n",
        "            \"Knowledge retention analysis\",\n",
        "            \"Adaptation speed measurements\",\n",
        "            \"Learning efficiency metrics\"\n",
        "        ],\n",
        "        \"Error Analysis\": [\n",
        "            \"Error frequency over time\",\n",
        "            \"Performance vs error relationship plots\",\n",
        "            \"Confidence miscalibration analysis\",\n",
        "            \"Error recovery time distribution\",\n",
        "            \"Performance degradation patterns\",\n",
        "            \"Error clustering analysis\"\n",
        "        ],\n",
        "        \"3D Visualizations\": [\n",
        "            \"3D performance space clustering\",\n",
        "            \"3D learning trajectory over time\",\n",
        "            \"Multi-dimensional scatter plots\",\n",
        "            \"Interactive 3D surface plots\"\n",
        "        ],\n",
        "        \"Advanced Analytics\": [\n",
        "            \"Network analysis of component interactions\",\n",
        "            \"Machine learning clustering analysis\",\n",
        "            \"Time series forecasting with confidence bands\",\n",
        "            \"Anomaly detection (statistical + ML)\",\n",
        "            \"Causal relationship analysis\",\n",
        "            \"Performance prediction models\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    for category, techniques in viz_types.items():\n",
        "        print(f\"\\nüìà {category}:\")\n",
        "        for technique in techniques:\n",
        "            print(f\"   ‚Ä¢ {technique}\")\n",
        "\n",
        "    print(f\"\\nüéØ Key Features:\")\n",
        "    print(\"   ‚Ä¢ Real-time performance tracking\")\n",
        "    print(\"   ‚Ä¢ Multi-dimensional analysis\")\n",
        "    print(\"   ‚Ä¢ Predictive analytics\")\n",
        "    print(\"   ‚Ä¢ Interactive visualizations\")\n",
        "    print(\"   ‚Ä¢ Automated insights generation\")\n",
        "    print(\"   ‚Ä¢ Comprehensive reporting\")\n",
        "\n",
        "def demonstrate_specific_visualization(viz_type=\"all\"):\n",
        "    \"\"\"Demonstrate specific visualization types\"\"\"\n",
        "\n",
        "    # Create sample agent with data\n",
        "    agent = SelfLearningRAGAgent()\n",
        "\n",
        "    # Generate sample data\n",
        "    sample_queries = [\n",
        "        \"What is machine learning?\",\n",
        "        \"How do neural networks work?\",\n",
        "        \"Explain deep learning concepts\",\n",
        "        \"What is natural language processing?\",\n",
        "        \"How does computer vision work?\",\n",
        "        \"What are transformer models?\",\n",
        "        \"Explain reinforcement learning\",\n",
        "        \"What is transfer learning?\",\n",
        "        \"How do GANs work?\",\n",
        "        \"What is federated learning?\"\n",
        "    ]\n",
        "\n",
        "    print(f\"\\nüîÑ Generating sample data for visualization...\")\n",
        "\n",
        "    for query in sample_queries:\n",
        "        agent.process_query(query)\n",
        "\n",
        "    df = pd.DataFrame(agent.performance_history)\n",
        "\n",
        "    if viz_type == \"interactive\" or viz_type == \"all\":\n",
        "        print(\"\\nüìä Interactive Dashboard Visualizations:\")\n",
        "        agent._create_interactive_dashboard(df)\n",
        "\n",
        "    if viz_type == \"statistical\" or viz_type == \"all\":\n",
        "        print(\"\\nüìà Statistical Analysis Visualizations:\")\n",
        "        agent._create_statistical_analysis(df)\n",
        "\n",
        "    if viz_type == \"temporal\" or viz_type == \"all\":\n",
        "        print(\"\\n‚è∞ Temporal Analysis Visualizations:\")\n",
        "        agent._create_temporal_analysis(df)\n",
        "\n",
        "    if viz_type == \"learning\" or viz_type == \"all\":\n",
        "        print(\"\\nüß† Learning Analytics Visualizations:\")\n",
        "        agent._create_learning_analytics(df)\n",
        "\n",
        "    if viz_type == \"error\" or viz_type == \"all\":\n",
        "        print(\"\\nüîç Error Analysis Visualizations:\")\n",
        "        agent._create_error_analysis(df)\n",
        "\n",
        "    if viz_type == \"3d\" or viz_type == \"all\":\n",
        "        print(\"\\nüåê 3D Analysis Visualizations:\")\n",
        "        agent._create_3d_analysis(df)\n",
        "\n",
        "    if viz_type == \"advanced\" or viz_type == \"all\":\n",
        "        print(\"\\nüöÄ Advanced Analytics:\")\n",
        "        create_advanced_analytics_suite(agent)\n",
        "\n",
        "def create_custom_visualization_examples():\n",
        "    \"\"\"Create custom visualization examples for specific use cases\"\"\"\n",
        "\n",
        "    print(\"\\nüé® Custom Visualization Examples\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Generate synthetic performance data for demonstration\n",
        "    np.random.seed(42)\n",
        "    n_queries = 50\n",
        "\n",
        "    # Simulate realistic performance evolution\n",
        "    base_performance = 0.4\n",
        "    improvement_rate = 0.01\n",
        "    noise_level = 0.1\n",
        "\n",
        "    performance_data = []\n",
        "    retrieval_data = []\n",
        "    reasoning_data = []\n",
        "    confidence_data = []\n",
        "    error_data = []\n",
        "\n",
        "    for i in range(n_queries):\n",
        "        # Simulate learning with some randomness\n",
        "        trend = base_performance + i * improvement_rate\n",
        "        noise = np.random.normal(0, noise_level)\n",
        "        performance = max(0.1, min(0.95, trend + noise))\n",
        "\n",
        "        retrieval = max(0.1, min(0.9, performance + np.random.normal(0, 0.05)))\n",
        "        reasoning = max(0.1, min(0.9, performance + np.random.normal(0, 0.05)))\n",
        "        confidence = max(0.1, min(0.9, performance + np.random.normal(0, 0.08)))\n",
        "\n",
        "        # Simulate occasional errors\n",
        "        error = np.random.random() < (0.2 - i * 0.003)  # Decreasing error rate\n",
        "\n",
        "        performance_data.append(performance)\n",
        "        retrieval_data.append(retrieval)\n",
        "        reasoning_data.append(reasoning)\n",
        "        confidence_data.append(confidence)\n",
        "        error_data.append(error)\n",
        "\n",
        "    # Create comprehensive dashboard\n",
        "    fig = make_subplots(\n",
        "        rows=3, cols=3,\n",
        "        subplot_titles=(\n",
        "            'Performance Evolution', 'Component Radar Chart', 'Quality Heatmap',\n",
        "            'Learning Acceleration', 'Error Pattern Analysis', 'Confidence Calibration',\n",
        "            'Performance Distribution', 'Trend Decomposition', 'Predictive Forecast'\n",
        "        ),\n",
        "        specs=[\n",
        "            [{\"secondary_y\": True}, {\"type\": \"polar\"}, {\"type\": \"heatmap\"}],\n",
        "            [{\"secondary_y\": False}, {\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "            [{\"secondary_y\": False}, {\"secondary_y\": False}, {\"secondary_y\": True}]\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # 1. Performance Evolution with Error Overlay\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=list(range(n_queries)), y=performance_data,\n",
        "                  mode='lines+markers', name='Overall Performance',\n",
        "                  line=dict(color='blue', width=3)),\n",
        "        row=1, col=1, secondary_y=False\n",
        "    )\n",
        "\n",
        "    # Add error indicators\n",
        "    error_indices = [i for i, e in enumerate(error_data) if e]\n",
        "    error_values = [performance_data[i] for i in error_indices]\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=error_indices, y=error_values,\n",
        "                  mode='markers', name='Errors',\n",
        "                  marker=dict(color='red', size=12, symbol='x')),\n",
        "        row=1, col=1, secondary_y=False\n",
        "    )\n",
        "\n",
        "    # Add moving average\n",
        "    window = 5\n",
        "    ma = pd.Series(performance_data).rolling(window=window).mean()\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=list(range(n_queries)), y=ma,\n",
        "                  mode='lines', name=f'MA({window})',\n",
        "                  line=dict(color='orange', width=2, dash='dash')),\n",
        "        row=1, col=1, secondary_y=False\n",
        "    )\n",
        "\n",
        "    # 2. Radar Chart for Latest Performance\n",
        "    categories = ['Overall', 'Retrieval', 'Reasoning', 'Confidence']\n",
        "    latest_values = [\n",
        "        performance_data[-1], retrieval_data[-1],\n",
        "        reasoning_data[-1], confidence_data[-1]\n",
        "    ]\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatterpolar(\n",
        "            r=latest_values + [latest_values[0]],\n",
        "            theta=categories + [categories[0]],\n",
        "            fill='toself', name='Current Performance',\n",
        "            line=dict(color='green', width=2)\n",
        "        ),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "    # Add comparison with average\n",
        "    avg_values = [\n",
        "        np.mean(performance_data), np.mean(retrieval_data),\n",
        "        np.mean(reasoning_data), np.mean(confidence_data)\n",
        "    ]\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatterpolar(\n",
        "            r=avg_values + [avg_values[0]],\n",
        "            theta=categories + [categories[0]],\n",
        "            fill='toself', name='Average Performance',\n",
        "            line=dict(color='red', width=2, dash='dot'),\n",
        "            opacity=0.6\n",
        "        ),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "    # 3. Performance Heatmap Over Time\n",
        "    # Create performance matrix (queries vs components)\n",
        "    performance_matrix = np.array([\n",
        "        performance_data, retrieval_data, reasoning_data, confidence_data\n",
        "    ])\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Heatmap(\n",
        "            z=performance_matrix,\n",
        "            y=['Overall', 'Retrieval', 'Reasoning', 'Confidence'],\n",
        "            x=list(range(0, n_queries, 5)),  # Sample every 5 queries\n",
        "            colorscale='RdYlGn',\n",
        "            showscale=True\n",
        "        ),\n",
        "        row=1, col=3\n",
        "    )\n",
        "\n",
        "    # 4. Learning Acceleration (Second Derivative)\n",
        "    acceleration = pd.Series(performance_data).diff().diff().rolling(3).mean()\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=list(range(n_queries)), y=acceleration,\n",
        "                  mode='lines+markers', name='Learning Acceleration',\n",
        "                  line=dict(color='purple', width=2)),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=list(range(n_queries)), y=[0]*n_queries,\n",
        "                  mode='lines', name='Zero Line',\n",
        "                  line=dict(color='black', width=1, dash='dash')),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "    # 5. Error Pattern Analysis\n",
        "    # Create error windows\n",
        "    error_windows = []\n",
        "    window_performance = []\n",
        "\n",
        "    for i in range(0, n_queries-5, 5):\n",
        "        window_errors = sum(error_data[i:i+5])\n",
        "        window_perf = np.mean(performance_data[i:i+5])\n",
        "        error_windows.append(window_errors)\n",
        "        window_performance.append(window_perf)\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=error_windows, y=window_performance,\n",
        "                  mode='markers', name='Error-Performance Relationship',\n",
        "                  marker=dict(size=10, color='red', opacity=0.6)),\n",
        "        row=2, col=2\n",
        "    )\n",
        "\n",
        "    # Add trend line\n",
        "    if len(error_windows) > 1:\n",
        "        z = np.polyfit(error_windows, window_performance, 1)\n",
        "        p = np.poly1d(z)\n",
        "        trend_x = np.linspace(0, max(error_windows), 100)\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=trend_x, y=p(trend_x),\n",
        "                      mode='lines', name='Trend',\n",
        "                      line=dict(color='blue', width=2, dash='dash')),\n",
        "            row=2, col=2\n",
        "        )\n",
        "\n",
        "    # 6. Confidence Calibration\n",
        "    # Create confidence bins\n",
        "    conf_bins = pd.cut(confidence_data, bins=5)\n",
        "    calibration_data = pd.DataFrame({\n",
        "        'confidence': confidence_data,\n",
        "        'performance': performance_data,\n",
        "        'bins': conf_bins\n",
        "    }).groupby('bins').agg({\n",
        "        'confidence': 'mean',\n",
        "        'performance': 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=[0, 1], y=[0, 1],\n",
        "                  mode='lines', name='Perfect Calibration',\n",
        "                  line=dict(color='gray', width=1, dash='dash')),\n",
        "        row=2, col=3\n",
        "    )\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=calibration_data['confidence'], y=calibration_data['performance'],\n",
        "                  mode='markers+lines', name='Actual Calibration',\n",
        "                  marker=dict(size=8, color='blue')),\n",
        "        row=2, col=3\n",
        "    )\n",
        "\n",
        "    # 7. Performance Distribution with Statistics\n",
        "    fig.add_trace(\n",
        "        go.Histogram(x=performance_data, nbinsx=15, name='Performance Distribution',\n",
        "                    marker_color='lightblue', opacity=0.7),\n",
        "        row=3, col=1\n",
        "    )\n",
        "\n",
        "    # Add statistical lines\n",
        "    mean_perf = np.mean(performance_data)\n",
        "    std_perf = np.std(performance_data)\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=[mean_perf, mean_perf], y=[0, 8],\n",
        "                  mode='lines', name=f'Mean ({mean_perf:.3f})',\n",
        "                  line=dict(color='red', width=2)),\n",
        "        row=3, col=1\n",
        "    )\n",
        "\n",
        "    # 8. Trend Decomposition\n",
        "    # Simple trend decomposition\n",
        "    trend = pd.Series(performance_data).rolling(window=10, center=True).mean()\n",
        "    detrended = pd.Series(performance_data) - trend\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=list(range(n_queries)), y=performance_data,\n",
        "                  mode='lines', name='Original',\n",
        "                  line=dict(color='blue', width=1)),\n",
        "        row=3, col=2\n",
        "    )\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=list(range(n_queries)), y=trend,\n",
        "                  mode='lines', name='Trend',\n",
        "                  line=dict(color='red', width=2)),\n",
        "        row=3, col=2\n",
        "    )\n",
        "\n",
        "    # 9. Predictive Forecast\n",
        "    # Simple linear regression forecast\n",
        "    X = np.array(range(n_queries)).reshape(-1, 1)\n",
        "    y = np.array(performance_data)\n",
        "\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    model = LinearRegression()\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Predict next 10 points\n",
        "    future_X = np.array(range(n_queries, n_queries + 10)).reshape(-1, 1)\n",
        "    future_pred = model.predict(future_X)\n",
        "\n",
        "    # Historical data\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=list(range(n_queries)), y=performance_data,\n",
        "                  mode='lines+markers', name='Historical',\n",
        "                  line=dict(color='blue', width=2)),\n",
        "        row=3, col=3, secondary_y=False\n",
        "    )\n",
        "\n",
        "    # Forecast\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=list(range(n_queries, n_queries + 10)), y=future_pred,\n",
        "                  mode='lines+markers', name='Forecast',\n",
        "                  line=dict(color='red', width=2, dash='dash')),\n",
        "        row=3, col=3, secondary_y=False\n",
        "    )\n",
        "\n",
        "    # Add confidence bands\n",
        "    residuals = y - model.predict(X)\n",
        "    std_residual = np.std(residuals)\n",
        "\n",
        "    upper_bound = future_pred + 1.96 * std_residual\n",
        "    lower_bound = future_pred - 1.96 * std_residual\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=list(range(n_queries, n_queries + 10)), y=upper_bound,\n",
        "                  mode='lines', line=dict(width=0), showlegend=False),\n",
        "        row=3, col=3, secondary_y=False\n",
        "    )\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=list(range(n_queries, n_queries + 10)), y=lower_bound,\n",
        "                  mode='lines', fill='tonexty', line=dict(width=0),\n",
        "                  fillcolor='rgba(255,0,0,0.1)', name='95% Confidence'),\n",
        "        row=3, col=3, secondary_y=False\n",
        "    )\n",
        "\n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        height=1200,\n",
        "        title_text=\"üé® Comprehensive RAG Agent Analytics Dashboard\",\n",
        "        showlegend=True\n",
        "    )\n",
        "\n",
        "    # Update specific subplot layouts\n",
        "    fig.update_polars(radialaxis=dict(range=[0, 1], tickmode='linear', tick0=0, dtick=0.2))\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "    # Performance insights\n",
        "    print(f\"\\nüìä Performance Insights from Custom Visualizations:\")\n",
        "    print(f\"   ‚Ä¢ Overall improvement trend: {model.coef_[0]*50:.3f} over 50 queries\")\n",
        "    print(f\"   ‚Ä¢ Performance consistency (std): {std_perf:.3f}\")\n",
        "    print(f\"   ‚Ä¢ Error rate decreased from {np.mean(error_data[:10]):.1%} to {np.mean(error_data[-10:]):.1%}\")\n",
        "    print(f\"   ‚Ä¢ Learning acceleration: {'Positive' if acceleration.mean() > 0 else 'Negative'}\")\n",
        "    print(f\"   ‚Ä¢ Confidence calibration: {'Well-calibrated' if abs(np.mean(confidence_data) - np.mean(performance_data)) < 0.1 else 'Needs adjustment'}\")\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    # Show visualization showcase first\n",
        "    create_visualization_showcase()\n",
        "\n",
        "    # Run full demonstration\n",
        "    agent, results, analytics, report = main()\n",
        "\n",
        "    # Optional: Run specific visualization demos\n",
        "    print(f\"\\nüéØ Want to see specific visualization types?\")\n",
        "    print(\"Uncomment the following lines to run specific demos:\")\n",
        "    print(\"# demonstrate_specific_visualization('interactive')\")\n",
        "    print(\"# demonstrate_specific_visualization('statistical')\")\n",
        "    print(\"# demonstrate_specific_visualization('learning')\")\n",
        "    print(\"# create_custom_visualization_examples()\")\n",
        "\n",
        "    # Uncomment to run specific examples:\n",
        "    # demonstrate_specific_visualization('interactive')\n",
        "    # create_custom_visualization_examples() \"# Self-Learning RAG Agent with Z-Reasoning Implementation\n",
        "# Compatible with Google Colab\n",
        "\n",
        "# First, install required packages\n",
        "# Run this in Colab first:\n",
        "\"\"\"\n",
        "!pip install langchain langchain-community langchain-openai\n",
        "!pip install chromadb sentence-transformers\n",
        "!pip install plotly matplotlib seaborn pandas numpy\n",
        "!pip install sqlite3 json datetime\n",
        "!pip install transformers torch\n",
        "!pip install openai tiktoken\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import sqlite3\n",
        "import datetime\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# LangChain imports\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "\n",
        "# For demo purposes, we'll create a mock LLM\n",
        "class MockLLM(LLM):\n",
        "    \"\"\"Mock LLM for demonstration purposes\"\"\"\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"mock\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "    ) -> str:\n",
        "        # Simulate different types of responses based on prompt content\n",
        "        if \"what is\" in prompt.lower():\n",
        "            return f\"Based on the retrieved context, here's my analysis: {prompt[:100]}...\"\n",
        "        elif \"critique\" in prompt.lower():\n",
        "            return f\"Analysis shows potential improvements in retrieval accuracy and reasoning depth.\"\n",
        "        else:\n",
        "            return f\"Processed response for: {prompt[:50]}...\"\n",
        "\n",
        "class MemoryManager:\n",
        "    \"\"\"Manages long-term memory storage for the RAG agent\"\"\"\n",
        "\n",
        "    def __init__(self, db_path: str = \"rag_memory.db\"):\n",
        "        self.db_path = db_path\n",
        "        self.init_database()\n",
        "\n",
        "    def init_database(self):\n",
        "        \"\"\"Initialize SQLite database for storing agent memory\"\"\"\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Create tables for different types of memory\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS interactions (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                timestamp TEXT,\n",
        "                user_query TEXT,\n",
        "                retrieved_docs TEXT,\n",
        "                reasoning_path TEXT,\n",
        "                final_answer TEXT,\n",
        "                feedback_score REAL,\n",
        "                hallucination_detected BOOLEAN,\n",
        "                retrieval_accuracy REAL\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS retrieval_strategies (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                strategy_name TEXT,\n",
        "                parameters TEXT,\n",
        "                success_rate REAL,\n",
        "                avg_relevance REAL,\n",
        "                created_at TEXT,\n",
        "                updated_at TEXT\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS reasoning_patterns (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                pattern_type TEXT,\n",
        "                success_rate REAL,\n",
        "                failure_modes TEXT,\n",
        "                improvement_suggestions TEXT,\n",
        "                usage_count INTEGER\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "\n",
        "    def store_interaction(self, interaction_data: Dict):\n",
        "        \"\"\"Store interaction data in memory\"\"\"\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        cursor.execute('''\n",
        "            INSERT INTO interactions\n",
        "            (timestamp, user_query, retrieved_docs, reasoning_path,\n",
        "             final_answer, feedback_score, hallucination_detected, retrieval_accuracy)\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
        "        ''', (\n",
        "            datetime.datetime.now().isoformat(),\n",
        "            interaction_data['user_query'],\n",
        "            json.dumps(interaction_data['retrieved_docs']),\n",
        "            interaction_data['reasoning_path'],\n",
        "            interaction_data['final_answer'],\n",
        "            interaction_data.get('feedback_score', 0.0),\n",
        "            interaction_data.get('hallucination_detected', False),\n",
        "            interaction_data.get('retrieval_accuracy', 0.0)\n",
        "        ))\n",
        "\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "\n",
        "    def get_recent_interactions(self, limit: int = 50) -> List[Dict]:\n",
        "        \"\"\"Retrieve recent interactions for analysis\"\"\"\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        cursor.execute('''\n",
        "            SELECT * FROM interactions\n",
        "            ORDER BY timestamp DESC\n",
        "            LIMIT ?\n",
        "        ''', (limit,))\n",
        "\n",
        "        rows = cursor.fetchall()\n",
        "        conn.close()\n",
        "\n",
        "        columns = ['id', 'timestamp', 'user_query', 'retrieved_docs',\n",
        "                  'reasoning_path', 'final_answer', 'feedback_score',\n",
        "                  'hallucination_detected', 'retrieval_accuracy']\n",
        "\n",
        "        return [dict(zip(columns, row)) for row in rows]\n",
        "\n",
        "class PlannerAgent:\n",
        "    \"\"\"Agent responsible for query planning and decomposition\"\"\"\n",
        "\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "\n",
        "    def plan_query(self, user_query: str) -> Dict[str, Any]:\n",
        "        \"\"\"Decompose user query into sub-tasks and planning steps\"\"\"\n",
        "        planning_prompt = f\"\"\"\n",
        "        Analyze this user query and create an execution plan:\n",
        "        Query: {user_query}\n",
        "\n",
        "        Provide:\n",
        "        1. Query type classification\n",
        "        2. Key information needs\n",
        "        3. Retrieval strategy recommendations\n",
        "        4. Reasoning approach\n",
        "        \"\"\"\n",
        "\n",
        "        plan = self.llm(planning_prompt)\n",
        "\n",
        "        return {\n",
        "            'original_query': user_query,\n",
        "            'query_type': self._classify_query_type(user_query),\n",
        "            'sub_queries': self._decompose_query(user_query),\n",
        "            'retrieval_strategy': 'semantic_search',\n",
        "            'reasoning_approach': 'chain_of_thought'\n",
        "        }\n",
        "\n",
        "    def _classify_query_type(self, query: str) -> str:\n",
        "        \"\"\"Classify the type of query\"\"\"\n",
        "        if any(word in query.lower() for word in ['what', 'define', 'explain']):\n",
        "            return 'factual'\n",
        "        elif any(word in query.lower() for word in ['how', 'why', 'analyze']):\n",
        "            return 'analytical'\n",
        "        elif any(word in query.lower() for word in ['compare', 'versus', 'difference']):\n",
        "            return 'comparative'\n",
        "        else:\n",
        "            return 'general'\n",
        "\n",
        "    def _decompose_query(self, query: str) -> List[str]:\n",
        "        \"\"\"Decompose complex queries into sub-queries\"\"\"\n",
        "        # Simple decomposition logic\n",
        "        if ' and ' in query:\n",
        "            return query.split(' and ')\n",
        "        elif '?' in query and len(query.split('?')) > 2:\n",
        "            return [q.strip() + '?' for q in query.split('?')[:-1]]\n",
        "        else:\n",
        "            return [query]\n",
        "\n",
        "class EnhancedRetriever:\n",
        "    \"\"\"Enhanced retriever with adaptive strategies\"\"\"\n",
        "\n",
        "    def __init__(self, vectorstore, memory_manager):\n",
        "        self.vectorstore = vectorstore\n",
        "        self.memory_manager = memory_manager\n",
        "        self.retrieval_strategies = {\n",
        "            'semantic_search': self._semantic_search,\n",
        "            'hybrid_search': self._hybrid_search,\n",
        "            'adaptive_search': self._adaptive_search\n",
        "        }\n",
        "\n",
        "    def retrieve(self, query: str, strategy: str = 'semantic_search',\n",
        "                k: int = 5) -> List[Document]:\n",
        "        \"\"\"Retrieve relevant documents using specified strategy\"\"\"\n",
        "        retrieval_func = self.retrieval_strategies.get(strategy, self._semantic_search)\n",
        "        return retrieval_func(query, k)\n",
        "\n",
        "    def _semantic_search(self, query: str, k: int) -> List[Document]:\n",
        "        \"\"\"Standard semantic similarity search\"\"\"\n",
        "        return self.vectorstore.similarity_search(query, k=k)\n",
        "\n",
        "    def _hybrid_search(self, query: str, k: int) -> List[Document]:\n",
        "        \"\"\"Hybrid search combining semantic and keyword matching\"\"\"\n",
        "        # For demo, we'll enhance with query expansion\n",
        "        expanded_queries = self._expand_query(query)\n",
        "        all_docs = []\n",
        "\n",
        "        for exp_query in expanded_queries:\n",
        "            docs = self.vectorstore.similarity_search(exp_query, k=k//len(expanded_queries) + 1)\n",
        "            all_docs.extend(docs)\n",
        "\n",
        "        # Remove duplicates and return top k\n",
        "        unique_docs = list({doc.page_content: doc for doc in all_docs}.values())\n",
        "        return unique_docs[:k]\n",
        "\n",
        "    def _adaptive_search(self, query: str, k: int) -> List[Document]:\n",
        "        \"\"\"Adaptive search based on past performance\"\"\"\n",
        "        # Analyze past performance for similar queries\n",
        "        recent_interactions = self.memory_manager.get_recent_interactions(20)\n",
        "\n",
        "        # Simple adaptation: adjust k based on past success\n",
        "        avg_accuracy = np.mean([i.get('retrieval_accuracy', 0.5)\n",
        "                               for i in recent_interactions])\n",
        "\n",
        "        if avg_accuracy < 0.3:\n",
        "            k = min(k + 3, 10)  # Retrieve more docs if accuracy is low\n",
        "        elif avg_accuracy > 0.8:\n",
        "            k = max(k - 1, 3)   # Retrieve fewer docs if accuracy is high\n",
        "\n",
        "        return self._hybrid_search(query, k)\n",
        "\n",
        "    def _expand_query(self, query: str) -> List[str]:\n",
        "        \"\"\"Expand query with synonyms and related terms\"\"\"\n",
        "        # Simple query expansion\n",
        "        base_queries = [query]\n",
        "\n",
        "        # Add question variations\n",
        "        if not query.endswith('?'):\n",
        "            base_queries.append(query + '?')\n",
        "\n",
        "        # Add \"what is\" variation for definitions\n",
        "        if 'what' not in query.lower():\n",
        "            base_queries.append(f\"what is {query}\")\n",
        "\n",
        "        return base_queries\n",
        "\n",
        "class ReasoningAgent:\n",
        "    \"\"\"Agent X: Primary reasoning with Chain of Thought\"\"\"\n",
        "\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "\n",
        "    def reason(self, query: str, retrieved_docs: List[Document],\n",
        "               reasoning_approach: str = 'chain_of_thought') -> Dict[str, Any]:\n",
        "        \"\"\"Perform reasoning on retrieved documents\"\"\"\n",
        "\n",
        "        context = self._format_context(retrieved_docs)\n",
        "\n",
        "        if reasoning_approach == 'chain_of_thought':\n",
        "            return self._chain_of_thought_reasoning(query, context)\n",
        "        elif reasoning_approach == 'step_by_step':\n",
        "            return self._step_by_step_reasoning(query, context)\n",
        "        else:\n",
        "            return self._basic_reasoning(query, context)\n",
        "\n",
        "    def _format_context(self, docs: List[Document]) -> str:\n",
        "        \"\"\"Format retrieved documents as context\"\"\"\n",
        "        context_parts = []\n",
        "        for i, doc in enumerate(docs, 1):\n",
        "            context_parts.append(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
        "        return \"\\n\".join(context_parts)\n",
        "\n",
        "    def _chain_of_thought_reasoning(self, query: str, context: str) -> Dict[str, Any]:\n",
        "        \"\"\"Chain of thought reasoning\"\"\"\n",
        "        cot_prompt = f\"\"\"\n",
        "        Context: {context}\n",
        "\n",
        "        Query: {query}\n",
        "\n",
        "        Let me think through this step by step:\n",
        "\n",
        "        Step 1: What is the user asking?\n",
        "        Step 2: What relevant information do I have?\n",
        "        Step 3: How can I connect this information to answer the query?\n",
        "        Step 4: What is my final answer?\n",
        "\n",
        "        Reasoning:\n",
        "        \"\"\"\n",
        "\n",
        "        reasoning_output = self.llm(cot_prompt)\n",
        "\n",
        "        return {\n",
        "            'reasoning_type': 'chain_of_thought',\n",
        "            'reasoning_steps': reasoning_output,\n",
        "            'final_answer': self._extract_final_answer(reasoning_output),\n",
        "            'confidence': self._estimate_confidence(reasoning_output, context)\n",
        "        }\n",
        "\n",
        "    def _step_by_step_reasoning(self, query: str, context: str) -> Dict[str, Any]:\n",
        "        \"\"\"Step by step reasoning\"\"\"\n",
        "        steps_prompt = f\"\"\"\n",
        "        Break down the solution to this query step by step:\n",
        "\n",
        "        Context: {context}\n",
        "        Query: {query}\n",
        "\n",
        "        Step-by-step analysis:\n",
        "        \"\"\"\n",
        "\n",
        "        reasoning_output = self.llm(steps_prompt)\n",
        "\n",
        "        return {\n",
        "            'reasoning_type': 'step_by_step',\n",
        "            'reasoning_steps': reasoning_output,\n",
        "            'final_answer': self._extract_final_answer(reasoning_output),\n",
        "            'confidence': self._estimate_confidence(reasoning_output, context)\n",
        "        }\n",
        "\n",
        "    def _basic_reasoning(self, query: str, context: str) -> Dict[str, Any]:\n",
        "        \"\"\"Basic reasoning without explicit structure\"\"\"\n",
        "        basic_prompt = f\"\"\"\n",
        "        Context: {context}\n",
        "        Query: {query}\n",
        "\n",
        "        Answer:\n",
        "        \"\"\"\n",
        "\n",
        "        reasoning_output = self.llm(basic_prompt)\n",
        "\n",
        "        return {\n",
        "            'reasoning_type': 'basic',\n",
        "            'reasoning_steps': reasoning_output,\n",
        "            'final_answer': reasoning_output,\n",
        "            'confidence': self._estimate_confidence(reasoning_output, context)\n",
        "        }\n",
        "\n",
        "    def _extract_final_answer(self, reasoning_output: str) -> str:\n",
        "        \"\"\"Extract final answer from reasoning output\"\"\"\n",
        "        # Simple extraction logic\n",
        "        if \"final answer:\" in reasoning_output.lower():\n",
        "            return reasoning_output.split(\"final answer:\")[-1].strip()\n",
        "        return reasoning_output.split('\\n')[-1].strip()\n",
        "\n",
        "    def _estimate_confidence(self, reasoning_output: str, context: str) -> float:\n",
        "        \"\"\"Estimate confidence in the reasoning\"\"\"\n",
        "        # Simple confidence estimation\n",
        "        confidence_indicators = ['certain', 'clear', 'definite', 'obvious']\n",
        "        uncertainty_indicators = ['might', 'could', 'possibly', 'uncertain']\n",
        "\n",
        "        conf_score = sum(1 for indicator in confidence_indicators\n",
        "                        if indicator in reasoning_output.lower())\n",
        "        unconf_score = sum(1 for indicator in uncertainty_indicators\n",
        "                          if indicator in reasoning_output.lower())\n",
        "\n",
        "        base_confidence = 0.5\n",
        "        confidence_adjustment = (conf_score - unconf_score) * 0.1\n",
        "\n",
        "        return max(0.1, min(0.9, base_confidence + confidence_adjustment))\n",
        "\n",
        "class ToolAgent:\n",
        "    \"\"\"Agent Y: External tool usage and data access\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.available_tools = {\n",
        "            'search_web': self._mock_web_search,\n",
        "            'query_database': self._mock_database_query,\n",
        "            'calculate': self._mock_calculator,\n",
        "            'fact_check': self._mock_fact_check\n",
        "        }\n",
        "\n",
        "    def use_tool(self, tool_name: str, query: str, **kwargs) -> Dict[str, Any]:\n",
        "        \"\"\"Use external tool to gather additional information\"\"\"\n",
        "        if tool_name not in self.available_tools:\n",
        "            return {'error': f'Tool {tool_name} not available'}\n",
        "\n",
        "        tool_func = self.available_tools[tool_name]\n",
        "        return tool_func(query, **kwargs)\n",
        "\n",
        "    def _mock_web_search(self, query: str, **kwargs) -> Dict[str, Any]:\n",
        "        \"\"\"Mock web search tool\"\"\"\n",
        "        return {\n",
        "            'tool': 'web_search',\n",
        "            'query': query,\n",
        "            'results': [\n",
        "                {'title': f'Result 1 for {query}', 'content': f'Mock content about {query}'},\n",
        "                {'title': f'Result 2 for {query}', 'content': f'Additional information on {query}'}\n",
        "            ],\n",
        "            'timestamp': datetime.datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "    def _mock_database_query(self, query: str, **kwargs) -> Dict[str, Any]:\n",
        "        \"\"\"Mock database query tool\"\"\"\n",
        "        return {\n",
        "            'tool': 'database_query',\n",
        "            'query': query,\n",
        "            'results': {'data': f'Database result for {query}', 'rows': 5},\n",
        "            'timestamp': datetime.datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "    def _mock_calculator(self, query: str, **kwargs) -> Dict[str, Any]:\n",
        "        \"\"\"Mock calculator tool\"\"\"\n",
        "        return {\n",
        "            'tool': 'calculator',\n",
        "            'expression': query,\n",
        "            'result': '42',  # Mock result\n",
        "            'timestamp': datetime.datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "    def _mock_fact_check(self, query: str, **kwargs) -> Dict[str, Any]:\n",
        "        \"\"\"Mock fact checking tool\"\"\"\n",
        "        return {\n",
        "            'tool': 'fact_check',\n",
        "            'statement': query,\n",
        "            'verification_status': 'partially_verified',\n",
        "            'confidence': 0.75,\n",
        "            'sources': ['Source A', 'Source B'],\n",
        "            'timestamp': datetime.datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "class CriticAgent:\n",
        "    \"\"\"Agent Z: Meta-critic for self-assessment and learning\"\"\"\n",
        "\n",
        "    def __init__(self, llm, memory_manager):\n",
        "        self.llm = llm\n",
        "        self.memory_manager = memory_manager\n",
        "\n",
        "    def critique(self, interaction_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Perform comprehensive critique of the interaction\"\"\"\n",
        "\n",
        "        critique_results = {\n",
        "            'retrieval_critique': self._critique_retrieval(interaction_data),\n",
        "            'reasoning_critique': self._critique_reasoning(interaction_data),\n",
        "            'answer_quality': self._assess_answer_quality(interaction_data),\n",
        "            'hallucination_check': self._check_hallucination(interaction_data),\n",
        "            'improvement_suggestions': self._generate_improvements(interaction_data)\n",
        "        }\n",
        "\n",
        "        # Calculate overall performance score\n",
        "        critique_results['overall_score'] = self._calculate_overall_score(critique_results)\n",
        "\n",
        "        return critique_results\n",
        "\n",
        "    def _critique_retrieval(self, interaction_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Critique retrieval quality and relevance\"\"\"\n",
        "        query = interaction_data['user_query']\n",
        "        retrieved_docs = interaction_data['retrieved_docs']\n",
        "\n",
        "        # Simple relevance scoring\n",
        "        relevance_scores = []\n",
        "        for doc in retrieved_docs:\n",
        "            # Mock relevance calculation\n",
        "            relevance = self._calculate_relevance(query, doc['page_content'])\n",
        "            relevance_scores.append(relevance)\n",
        "\n",
        "        return {\n",
        "            'avg_relevance': np.mean(relevance_scores) if relevance_scores else 0.0,\n",
        "            'relevance_scores': relevance_scores,\n",
        "            'coverage_assessment': self._assess_coverage(query, retrieved_docs),\n",
        "            'diversity_score': self._calculate_diversity(retrieved_docs)\n",
        "        }\n",
        "\n",
        "    def _critique_reasoning(self, interaction_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Critique reasoning quality and logical consistency\"\"\"\n",
        "        reasoning_path = interaction_data['reasoning_path']\n",
        "\n",
        "        critique_prompt = f\"\"\"\n",
        "        Analyze this reasoning for logical consistency and quality:\n",
        "\n",
        "        Reasoning: {reasoning_path}\n",
        "\n",
        "        Assess:\n",
        "        1. Logical consistency\n",
        "        2. Use of evidence\n",
        "        3. Clarity of reasoning\n",
        "        4. Potential gaps or errors\n",
        "        \"\"\"\n",
        "\n",
        "        reasoning_critique = self.llm(critique_prompt)\n",
        "\n",
        "        return {\n",
        "            'logical_consistency': self._score_logical_consistency(reasoning_path),\n",
        "            'evidence_usage': self._score_evidence_usage(reasoning_path),\n",
        "            'clarity_score': self._score_clarity(reasoning_path),\n",
        "            'detailed_critique': reasoning_critique\n",
        "        }\n",
        "\n",
        "    def _assess_answer_quality(self, interaction_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Assess the quality of the final answer\"\"\"\n",
        "        final_answer = interaction_data['final_answer']\n",
        "        user_query = interaction_data['user_query']\n",
        "\n",
        "        return {\n",
        "            'completeness': self._score_completeness(user_query, final_answer),\n",
        "            'accuracy_estimate': self._estimate_accuracy(final_answer),\n",
        "            'clarity': self._score_answer_clarity(final_answer),\n",
        "            'specificity': self._score_specificity(final_answer)\n",
        "        }\n",
        "\n",
        "    def _check_hallucination(self, interaction_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Check for potential hallucinations in the answer\"\"\"\n",
        "        final_answer = interaction_data['final_answer']\n",
        "        retrieved_docs = interaction_data['retrieved_docs']\n",
        "\n",
        "        # Simple hallucination detection\n",
        "        hallucination_indicators = self._detect_hallucination_patterns(final_answer, retrieved_docs)\n",
        "\n",
        "        return {\n",
        "            'hallucination_detected': len(hallucination_indicators) > 0,\n",
        "            'hallucination_score': len(hallucination_indicators) / 10.0,\n",
        "            'indicators': hallucination_indicators,\n",
        "            'confidence': 0.7  # Mock confidence in detection\n",
        "        }\n",
        "\n",
        "    def _generate_improvements(self, interaction_data: Dict[str, Any]) -> List[str]:\n",
        "        \"\"\"Generate specific improvement suggestions\"\"\"\n",
        "        improvements = []\n",
        "\n",
        "        # Based on critique results, suggest improvements\n",
        "        retrieval_quality = interaction_data.get('retrieval_critique', {}).get('avg_relevance', 0.5)\n",
        "        if retrieval_quality < 0.6:\n",
        "            improvements.append(\"Consider expanding query or using hybrid retrieval strategy\")\n",
        "\n",
        "        reasoning_quality = interaction_data.get('reasoning_critique', {}).get('logical_consistency', 0.5)\n",
        "        if reasoning_quality < 0.6:\n",
        "            improvements.append(\"Strengthen logical reasoning with more explicit steps\")\n",
        "\n",
        "        if interaction_data.get('hallucination_check', {}).get('hallucination_detected', False):\n",
        "            improvements.append(\"Implement stronger grounding in retrieved context\")\n",
        "\n",
        "        return improvements\n",
        "\n",
        "    def _calculate_overall_score(self, critique_results: Dict[str, Any]) -> float:\n",
        "        \"\"\"Calculate overall performance score\"\"\"\n",
        "        scores = []\n",
        "\n",
        "        if 'retrieval_critique' in critique_results:\n",
        "            scores.append(critique_results['retrieval_critique'].get('avg_relevance', 0.5))\n",
        "\n",
        "        if 'reasoning_critique' in critique_results:\n",
        "            scores.append(critique_results['reasoning_critique'].get('logical_consistency', 0.5))\n",
        "\n",
        "        if 'answer_quality' in critique_results:\n",
        "            scores.append(critique_results['answer_quality'].get('completeness', 0.5))\n",
        "\n",
        "        return np.mean(scores) if scores else 0.5\n",
        "\n",
        "    # Helper methods for scoring\n",
        "    def _calculate_relevance(self, query: str, doc_content: str) -> float:\n",
        "        \"\"\"Calculate relevance score between query and document\"\"\"\n",
        "        # Simple word overlap-based relevance\n",
        "        query_words = set(query.lower().split())\n",
        "        doc_words = set(doc_content.lower().split())\n",
        "\n",
        "        overlap = len(query_words.intersection(doc_words))\n",
        "        total_words = len(query_words.union(doc_words))\n",
        "\n",
        "        return overlap / total_words if total_words > 0 else 0.0\n",
        "\n",
        "    def _assess_coverage(self, query: str, retrieved_docs: List[Dict]) -> float:\n",
        "        \"\"\"Assess how well retrieved docs cover the query\"\"\"\n",
        "        # Mock coverage assessment\n",
        "        return 0.7\n",
        "\n",
        "    def _calculate_diversity(self, retrieved_docs: List[Dict]) -> float:\n",
        "        \"\"\"Calculate diversity of retrieved documents\"\"\"\n",
        "        # Mock diversity calculation\n",
        "        return 0.6\n",
        "\n",
        "    def _score_logical_consistency(self, reasoning: str) -> float:\n",
        "        \"\"\"Score logical consistency of reasoning\"\"\"\n",
        "        # Simple heuristic: longer, structured reasoning scores higher\n",
        "        if \"step\" in reasoning.lower() and len(reasoning) > 100:\n",
        "            return 0.8\n",
        "        return 0.5\n",
        "\n",
        "    def _score_evidence_usage(self, reasoning: str) -> float:\n",
        "        \"\"\"Score how well evidence is used in reasoning\"\"\"\n",
        "        evidence_indicators = ['according to', 'based on', 'document', 'source']\n",
        "        score = sum(1 for indicator in evidence_indicators if indicator in reasoning.lower())\n",
        "        return min(score / 4.0, 1.0)\n",
        "\n",
        "    def _score_clarity(self, reasoning: str) -> float:\n",
        "        \"\"\"Score clarity of reasoning\"\"\"\n",
        "        # Simple heuristic based on structure and length\n",
        "        return 0.7\n",
        "\n",
        "    def _score_completeness(self, query: str, answer: str) -> float:\n",
        "        \"\"\"Score completeness of answer relative to query\"\"\"\n",
        "        # Simple heuristic\n",
        "        return 0.6\n",
        "\n",
        "    def _estimate_accuracy(self, answer: str) -> float:\n",
        "        \"\"\"Estimate accuracy of answer\"\"\"\n",
        "        # Mock accuracy estimation\n",
        "        return 0.75\n",
        "\n",
        "    def _score_answer_clarity(self, answer: str) -> float:\n",
        "        \"\"\"Score clarity of final answer\"\"\"\n",
        "        return 0.8\n",
        "\n",
        "    def _score_specificity(self, answer: str) -> float:\n",
        "        \"\"\"Score specificity of answer\"\"\"\n",
        "        return 0.7\n",
        "\n",
        "    def _detect_hallucination_patterns(self, answer: str, retrieved_docs: List[Dict]) -> List[str]:\n",
        "        \"\"\"Detect potential hallucination patterns\"\"\"\n",
        "        indicators = []\n",
        "\n",
        "        # Check if answer contains information not in retrieved docs\n",
        "        answer_words = set(answer.lower().split())\n",
        "        doc_words = set()\n",
        "        for doc in retrieved_docs:\n",
        "            doc_words.update(doc.get('page_content', '').lower().split())\n",
        "\n",
        "        unsupported_words = answer_words - doc_words\n",
        "        if len(unsupported_words) > len(answer_words) * 0.3:\n",
        "            indicators.append(\"High proportion of unsupported information\")\n",
        "\n",
        "        return indicators\n",
        "\n",
        "class SelfLearningRAGAgent:\n",
        "    \"\"\"Main RAG Agent orchestrating all components\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Initialize components\n",
        "        self.llm = MockLLM()\n",
        "        self.memory_manager = MemoryManager()\n",
        "        self.planner = PlannerAgent(self.llm)\n",
        "        self.tool_agent = ToolAgent()\n",
        "        self.reasoning_agent = ReasoningAgent(self.llm)\n",
        "        self.critic_agent = CriticAgent(self.llm, self.memory_manager)\n",
        "\n",
        "        # Initialize vector store with sample documents\n",
        "        self.vectorstore = self._initialize_vectorstore()\n",
        "        self.retriever = EnhancedRetriever(self.vectorstore, self.memory_manager)\n",
        "\n",
        "        # Performance tracking\n",
        "        self.performance_history = []\n",
        "\n",
        "    def _initialize_vectorstore(self):\n",
        "        \"\"\"Initialize vector store with sample documents\"\"\"\n",
        "        # Sample documents for demonstration\n",
        "        sample_docs = [\n",
        "            \"Artificial Intelligence is the simulation of human intelligence in machines.\",\n",
        "            \"Machine Learning is a subset of AI that enables computers to learn without explicit programming.\",\n",
        "            \"Deep Learning uses neural networks with multiple layers to analyze data.\",\n",
        "            \"Natural Language Processing helps computers understand and generate human language.\",\n",
        "            \"Computer Vision enables machines to interpret and understand visual information.\",\n",
        "            \"Reinforcement Learning is a type of ML where agents learn through trial and error.\",\n",
        "            \"Large Language Models are AI systems trained on vast amounts of text data.\",\n",
        "            \"RAG (Retrieval Augmented Generation) combines retrieval with generation for better answers.\",\n",
        "        ]\n",
        "\n",
        "        # Create documents\n",
        "        documents = [Document(page_content=content) for content in sample_docs]\n",
        "\n",
        "        # Initialize embeddings and vector store\n",
        "        embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "        vectorstore = Chroma.from_documents(documents, embeddings)\n",
        "\n",
        "        return vectorstore\n",
        "\n",
        "    def process_query(self, user_query: str) -> Dict[str, Any]:\n",
        "        \"\"\"Main query processing pipeline\"\"\"\n",
        "        print(f\"Processing query: {user_query}\")\n",
        "\n",
        "        # Step 1: Planning\n",
        "        plan = self.planner.plan_query(user_query)\n",
        "        print(f\"Plan created: {plan['query_type']} query\")\n",
        "\n",
        "        # Step 2: Retrieval\n",
        "        retrieved_docs = self.retriever.retrieve(\n",
        "            user_query,\n",
        "            strategy=plan['retrieval_strategy'],\n",
        "            k=5\n",
        "        )\n",
        "        print(f\"Retrieved {len(retrieved_docs)} documents\")\n",
        "\n",
        "        # Step 3: Tool usage (if needed)\n",
        "        tool_results = []\n",
        "        if plan['query_type'] in ['analytical', 'comparative']:\n",
        "            tool_result = self.tool_agent.use_tool('fact_check', user_query)\n",
        "            tool_results.append(tool_result)\n",
        "\n",
        "        # Step 4: Reasoning\n",
        "        reasoning_result = self.reasoning_agent.reason(\n",
        "            user_query,\n",
        "            retrieved_docs,\n",
        "            plan['reasoning_approach']\n",
        "        )\n",
        "        print(f\"Reasoning completed with confidence: {reasoning_result['confidence']:.2f}\")\n",
        "\n",
        "        # Step 5: Prepare interaction data\n",
        "        interaction_data = {\n",
        "            'user_query': user_query,\n",
        "            'plan': plan,\n",
        "            'retrieved_docs': [{'page_content': doc.page_content} for doc in retrieved_docs],\n",
        "            'tool_results': tool_results,\n",
        "            'reasoning_path': reasoning_result['reasoning_steps'],\n",
        "            'final_answer': reasoning_result['final_answer'],\n",
        "            'confidence': reasoning_result['confidence']\n",
        "        }\n",
        "\n",
        "        # Step 6: Critique and learning\n",
        "        critique_result = self.critic_agent.critique(interaction_data)\n",
        "        print(f\"Critique completed. Overall score: {critique_result['overall_score']:.2f}\")\n",
        "\n",
        "        # Step 7: Store in memory\n",
        "        memory_data = {\n",
        "            **interaction_data,\n",
        "            'feedback_score': critique_result['overall_score'],\n",
        "            'hallucination_detected': critique_result['hallucination_check']['hallucination_detected'],\n",
        "            'retrieval_accuracy': critique_result['retrieval_critique']['avg_relevance']\n",
        "        }\n",
        "        self.memory_manager.store_interaction(memory_data)\n",
        "\n",
        "        # Step 8: Update performance tracking\n",
        "        self.performance_history.append({\n",
        "            'timestamp': datetime.datetime.now(),\n",
        "            'query': user_query,\n",
        "            'overall_score': critique_result['overall_score'],\n",
        "            'retrieval_accuracy': critique_result['retrieval_critique']['avg_relevance'],\n",
        "            'reasoning_quality': critique_result['reasoning_critique']['logical_consistency'],\n",
        "            'hallucination_detected': critique_result['hallucination_check']['hallucination_detected']\n",
        "        })\n",
        "\n",
        "        return {\n",
        "            'query': user_query,\n",
        "            'answer': reasoning_result['final_answer'],\n",
        "            'confidence': reasoning_result['confidence'],\n",
        "            'critique': critique_result,\n",
        "            'improvements': critique_result['improvement_suggestions']\n",
        "        }\n",
        "\n",
        "    def get_performance_analytics(self) -> Dict[str, Any]:\n",
        "        \"\"\"Generate comprehensive performance analytics\"\"\"\n",
        "        if not self.performance_history:\n",
        "            return {\"message\": \"No performance data available yet\"}\n",
        "\n",
        "        df = pd.DataFrame(self.performance_history)\n",
        "\n",
        "        analytics = {\n",
        "            'summary_stats': {\n",
        "                'total_queries': len(df),\n",
        "                'avg_overall_score': df['overall_score'].mean(),\n",
        "                'avg_retrieval_accuracy': df['retrieval_accuracy'].mean(),\n",
        "                'avg_reasoning_quality': df['reasoning_quality'].mean(),\n",
        "                'hallucination_rate': df['hallucination_detected'].sum() / len(df)\n",
        "            },\n",
        "            'trends': {\n",
        "                'score_improvement': self._calculate_trend(df, 'overall_score'),\n",
        "                'retrieval_improvement': self._calculate_trend(df, 'retrieval_accuracy'),\n",
        "                'reasoning_improvement': self._calculate_trend(df, 'reasoning_quality')\n",
        "            },\n",
        "            'recent_performance': df.tail(10).to_dict('records')\n",
        "        }\n",
        "\n",
        "        return analytics\n",
        "\n",
        "    def _calculate_trend(self, df: pd.DataFrame, column: str) -> float:\n",
        "        \"\"\"Calculate performance trend (positive = improving)\"\"\"\n",
        "        if len(df) < 2:\n",
        "            return 0.0\n",
        "\n",
        "        recent_avg = df.tail(5)[column].mean()\n",
        "        older_avg = df.head(5)[column].mean()\n",
        "\n",
        "        return recent_avg - older_avg\n",
        "\n",
        "    def visualize_performance(self):\n",
        "        \"\"\"Create comprehensive performance visualizations with multiple chart types\"\"\"\n",
        "        if not self.performance_history:\n",
        "            print(\"No performance data to visualize\")\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(self.performance_history)\n",
        "\n",
        "        # Call all visualization functions\n",
        "        self._create_interactive_dashboard(df)\n",
        "        self._create_statistical_analysis(df)\n",
        "        self._create_temporal_analysis(df)\n",
        "        self._create_comparative_analysis(df)\n",
        "        self._create_learning_analytics(df)\n",
        "        self._create_error_analysis(df)\n",
        "        self._create_3d_analysis(df)\n",
        "\n",
        "    def _create_interactive_dashboard(self, df):\n",
        "        \"\"\"Create interactive Plotly dashboard\"\"\"\n",
        "        fig = make_subplots(\n",
        "            rows=3, cols=2,\n",
        "            subplot_titles=('Performance Timeline', 'Component Radar Chart',\n",
        "                          'Quality Distribution', 'Learning Velocity',\n",
        "                          'Confidence vs Accuracy', 'Strategy Effectiveness'),\n",
        "            specs=[[{\"secondary_y\": True}, {\"type\": \"polar\"}],\n",
        "                   [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "                   [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        "        )\n",
        "\n",
        "        # 1. Multi-metric timeline with dual axis\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=list(range(len(df))), y=df['overall_score'],\n",
        "                      mode='lines+markers', name='Overall Score',\n",
        "                      line=dict(color='blue', width=3)),\n",
        "            row=1, col=1, secondary_y=False\n",
        "        )\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Bar(x=list(range(len(df))), y=df['hallucination_detected'].astype(int),\n",
        "                  name='Hallucinations', marker_color='red', opacity=0.3),\n",
        "            row=1, col=1, secondary_y=True\n",
        "        )\n",
        "\n",
        "        # 2. Radar chart for component performance\n",
        "        categories = ['Retrieval', 'Reasoning', 'Overall', 'Confidence']\n",
        "        values = [df['retrieval_accuracy'].mean(), df['reasoning_quality'].mean(),\n",
        "                 df['overall_score'].mean(), df['confidence'].mean()]\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Scatterpolar(r=values + [values[0]], theta=categories + [categories[0]],\n",
        "                           fill='toself', name='Average Performance',\n",
        "                           line=dict(color='green')),\n",
        "            row=1, col=2\n",
        "        )\n",
        "\n",
        "        # 3. Box plot for quality distribution\n",
        "        fig.add_trace(\n",
        "            go.Box(y=df['overall_score'], name='Overall', boxpoints='all',\n",
        "                  marker_color='blue'),\n",
        "            row=2, col=1\n",
        "        )\n",
        "        fig.add_trace(\n",
        "            go.Box(y=df['retrieval_accuracy'], name='Retrieval', boxpoints='all',\n",
        "                  marker_color='green'),\n",
        "            row=2, col=1\n",
        "        )\n",
        "\n",
        "        # 4. Learning velocity (performance change rate)\n",
        "        velocity = df['overall_score'].diff().rolling(window=3).mean()\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=list(range(len(df))), y=velocity,\n",
        "                      mode='lines+markers', name='Learning Velocity',\n",
        "                      line=dict(color='purple', width=2)),\n",
        "            row=2, col=2\n",
        "        )\n",
        "\n",
        "        # 5. Confidence vs Accuracy scatter with size encoding\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=df['confidence'], y=df['overall_score'],\n",
        "                      mode='markers', name='Queries',\n",
        "                      marker=dict(size=df['retrieval_accuracy']*50,\n",
        "                                 color=df['reasoning_quality'],\n",
        "                                 colorscale='Viridis', showscale=True)),\n",
        "            row=3, col=1\n",
        "        )\n",
        "\n",
        "        # 6. Strategy effectiveness over time\n",
        "        strategy_scores = df.groupby(df.index // 2)['overall_score'].mean()\n",
        "        fig.add_trace(\n",
        "            go.Bar(x=list(range(len(strategy_scores))), y=strategy_scores,\n",
        "                  name='Strategy Batches', marker_color='orange'),\n",
        "            row=3, col=2\n",
        "        )\n",
        "\n",
        "        fig.update_layout(height=1200, title_text=\"üöÄ Interactive RAG Agent Dashboard\")\n",
        "        fig.show()\n",
        "\n",
        "    def _create_statistical_analysis(self, df):\n",
        "        \"\"\"Create statistical analysis visualizations\"\"\"\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        fig.suptitle('üìä Statistical Performance Analysis', fontsize=16)\n",
        "\n",
        "        # 1. Distribution analysis with KDE\n",
        "        axes[0,0].hist(df['overall_score'], bins=15, alpha=0.7, color='skyblue',\n",
        "                      density=True, label='Histogram')\n",
        "        try:\n",
        "            from scipy import stats\n",
        "            kde_x = np.linspace(df['overall_score'].min(), df['overall_score'].max(), 100)\n",
        "            kde = stats.gaussian_kde(df['overall_score'])\n",
        "            axes[0,0].plot(kde_x, kde(kde_x), 'r-', linewidth=2, label='KDE')\n",
        "        except:\n",
        "            pass\n",
        "        axes[0,0].set_title('Score Distribution with Density')\n",
        "        axes[0,0].legend()\n",
        "\n",
        "        # 2. Correlation matrix with annotations\n",
        "        corr_data = df[['overall_score', 'retrieval_accuracy', 'reasoning_quality', 'confidence']].corr()\n",
        "        im = axes[0,1].imshow(corr_data, cmap='RdBu_r', vmin=-1, vmax=1)\n",
        "        axes[0,1].set_xticks(range(len(corr_data.columns)))\n",
        "        axes[0,1].set_yticks(range(len(corr_data.columns)))\n",
        "        axes[0,1].set_xticklabels(corr_data.columns, rotation=45)\n",
        "        axes[0,1].set_yticklabels(corr_data.columns)\n",
        "\n",
        "        # Add correlation values\n",
        "        for i in range(len(corr_data.columns)):\n",
        "            for j in range(len(corr_data.columns)):\n",
        "                text = axes[0,1].text(j, i, f'{corr_data.iloc[i, j]:.2f}',\n",
        "                                     ha=\"center\", va=\"center\", color=\"black\")\n",
        "        axes[0,1].set_title('Correlation Matrix')\n",
        "\n",
        "        # 3. Performance quartiles\n",
        "        quartiles = df['overall_score'].quantile([0.25, 0.5, 0.75])\n",
        "        performance_categories = pd.cut(df['overall_score'],\n",
        "                                      bins=[0, quartiles[0.25], quartiles[0.5], quartiles[0.75], 1],\n",
        "                                      labels=['Low', 'Medium', 'High', 'Excellent'])\n",
        "        performance_counts = performance_categories.value_counts()\n",
        "\n",
        "        axes[0,2].pie(performance_counts.values, labels=performance_counts.index,\n",
        "                     autopct='%1.1f%%', startangle=90, colors=['red', 'orange', 'lightgreen', 'green'])\n",
        "        axes[0,2].set_title('Performance Categories')\n",
        "\n",
        "        # 4. Confidence intervals over time\n",
        "        window_size = 5\n",
        "        rolling_mean = df['overall_score'].rolling(window=window_size).mean()\n",
        "        rolling_std = df['overall_score'].rolling(window=window_size).std()\n",
        "\n",
        "        x = range(len(df))\n",
        "        axes[1,0].plot(x, rolling_mean, 'b-', linewidth=2, label='Mean')\n",
        "        axes[1,0].fill_between(x, rolling_mean - rolling_std, rolling_mean + rolling_std,\n",
        "                              alpha=0.3, color='blue', label='¬±1 STD')\n",
        "        axes[1,0].scatter(x, df['overall_score'], alpha=0.5, s=20, color='red')\n",
        "        axes[1,0].set_title('Performance with Confidence Bands')\n",
        "        axes[1,0].legend()\n",
        "\n",
        "        # 5. Quality vs Quantity analysis\n",
        "        query_complexity = [len(q.split()) for q in df['query']]\n",
        "        axes[1,1].hexbin(query_complexity, df['overall_score'], gridsize=10, cmap='Blues')\n",
        "        axes[1,1].set_xlabel('Query Complexity (words)')\n",
        "        axes[1,1].set_ylabel('Performance Score')\n",
        "        axes[1,1].set_title('Complexity vs Performance (Hexbin)')\n",
        "\n",
        "        # 6. Outlier detection\n",
        "        Q1 = df['overall_score'].quantile(0.25)\n",
        "        Q3 = df['overall_score'].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        outliers = df[(df['overall_score'] < Q1 - 1.5*IQR) | (df['overall_score'] > Q3 + 1.5*IQR)]\n",
        "\n",
        "        axes[1,2].scatter(range(len(df)), df['overall_score'], alpha=0.6, color='blue', label='Normal')\n",
        "        if len(outliers) > 0:\n",
        "            outlier_indices = outliers.index\n",
        "            axes[1,2].scatter(outlier_indices, outliers['overall_score'],\n",
        "                             color='red', s=100, label='Outliers', marker='x')\n",
        "        axes[1,2].set_title('Outlier Detection')\n",
        "        axes[1,2].legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def _create_temporal_analysis(self, df):\n",
        "        \"\"\"Create time-series specific analysis\"\"\"\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=('Performance Decomposition', 'Seasonal Patterns',\n",
        "                          'Moving Averages Comparison', 'Volatility Analysis'),\n",
        "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "                   [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        "        )\n",
        "\n",
        "        # 1. STL Decomposition simulation\n",
        "        trend = np.polyval(np.polyfit(range(len(df)), df['overall_score'], 2), range(len(df)))\n",
        "        residuals = df['overall_score'] - trend\n",
        "\n",
        "        fig.add_trace(go.Scatter(x=list(range(len(df))), y=df['overall_score'],\n",
        "                                name='Original', line=dict(color='blue')), row=1, col=1)\n",
        "        fig.add_trace(go.Scatter(x=list(range(len(df))), y=trend,\n",
        "                                name='Trend', line=dict(color='red')), row=1, col=1)\n",
        "\n",
        "        # 2. Cyclical patterns (simulated)\n",
        "        cycle_length = 4\n",
        "        cycles = [i % cycle_length for i in range(len(df))]\n",
        "        cycle_performance = df.groupby(cycles)['overall_score'].mean()\n",
        "\n",
        "        fig.add_trace(go.Bar(x=list(range(cycle_length)), y=cycle_performance,\n",
        "                            name='Cycle Performance', marker_color='green'), row=1, col=2)\n",
        "\n",
        "        # 3. Multiple moving averages\n",
        "        ma_short = df['overall_score'].rolling(window=3).mean()\n",
        "        ma_long = df['overall_score'].rolling(window=7).mean()\n",
        "\n",
        "        fig.add_trace(go.Scatter(x=list(range(len(df))), y=df['overall_score'],\n",
        "                                name='Original', opacity=0.3), row=2, col=1)\n",
        "        fig.add_trace(go.Scatter(x=list(range(len(df))), y=ma_short,\n",
        "                                name='MA(3)', line=dict(color='orange')), row=2, col=1)\n",
        "        fig.add_trace(go.Scatter(x=list(range(len(df))), y=ma_long,\n",
        "                                name='MA(7)', line=dict(color='purple')), row=2, col=1)\n",
        "\n",
        "        # 4. Volatility (rolling standard deviation)\n",
        "        volatility = df['overall_score'].rolling(window=5).std()\n",
        "        fig.add_trace(go.Scatter(x=list(range(len(df))), y=volatility,\n",
        "                                mode='lines+markers', name='Volatility',\n",
        "                                line=dict(color='red')), row=2, col=2)\n",
        "\n",
        "        fig.update_layout(height=800, title_text=\"‚è∞ Temporal Analysis Dashboard\")\n",
        "        fig.show()\n",
        "\n",
        "    def _create_comparative_analysis(self, df):\n",
        "        \"\"\"Create comparative analysis between different aspects\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle('‚öñÔ∏è Comparative Performance Analysis', fontsize=16)\n",
        "\n",
        "        # 1. Before/After analysis\n",
        "        midpoint = len(df) // 2\n",
        "        early_data = df.iloc[:midpoint]\n",
        "        late_data = df.iloc[midpoint:]\n",
        "\n",
        "        comparison_data = {\n",
        "            'Period': ['Early', 'Late'],\n",
        "            'Overall Score': [early_data['overall_score'].mean(), late_data['overall_score'].mean()],\n",
        "            'Retrieval': [early_data['retrieval_accuracy'].mean(), late_data['retrieval_accuracy'].mean()],\n",
        "            'Reasoning': [early_data['reasoning_quality'].mean(), late_data['reasoning_quality'].mean()]\n",
        "        }\n",
        "\n",
        "        x = np.arange(len(comparison_data['Period']))\n",
        "        width = 0.25\n",
        "\n",
        "        axes[0,0].bar(x - width, comparison_data['Overall Score'], width, label='Overall Score')\n",
        "        axes[0,0].bar(x, comparison_data['Retrieval'], width, label='Retrieval')\n",
        "        axes[0,0].bar(x + width, comparison_data['Reasoning'], width, label='Reasoning')\n",
        "        axes[0,0].set_xlabel('Time Period')\n",
        "        axes[0,0].set_ylabel('Performance Score')\n",
        "        axes[0,0].set_title('Early vs Late Performance')\n",
        "        axes[0,0].set_xticks(x)\n",
        "        axes[0,0].set_xticklabels(comparison_data['Period'])\n",
        "        axes[0,0].legend()\n",
        "\n",
        "        # 2. Performance by query type (simulated)\n",
        "        query_types = ['Factual', 'Analytical', 'Comparative', 'General']\n",
        "        type_performance = [0.7, 0.65, 0.8, 0.6]  # Simulated data\n",
        "        type_counts = [len(df)//4] * 4\n",
        "\n",
        "        axes[0,1].scatter(type_counts, type_performance, s=200, alpha=0.7,\n",
        "                         c=['red', 'blue', 'green', 'orange'])\n",
        "        for i, txt in enumerate(query_types):\n",
        "            axes[0,1].annotate(txt, (type_counts[i], type_performance[i]))\n",
        "        axes[0,1].set_xlabel('Query Count')\n",
        "        axes[0,1].set_ylabel('Average Performance')\n",
        "        axes[0,1].set_title('Performance by Query Type')\n",
        "\n",
        "        # 3. Confidence calibration\n",
        "        confidence_bins = pd.cut(df['confidence'], bins=5)\n",
        "        calibration_data = df.groupby(confidence_bins).agg({\n",
        "            'overall_score': 'mean',\n",
        "            'confidence': 'mean'\n",
        "        }).reset_index(drop=True)\n",
        "\n",
        "        axes[1,0].plot([0, 1], [0, 1], 'r--', label='Perfect Calibration')\n",
        "        axes[1,0].scatter(calibration_data['confidence'], calibration_data['overall_score'],\n",
        "                         s=100, alpha=0.7, label='Actual Calibration')\n",
        "        axes[1,0].set_xlabel('Predicted Confidence')\n",
        "        axes[1,0].set_ylabel('Actual Performance')\n",
        "        axes[1,0].set_title('Confidence Calibration')\n",
        "        axes[1,0].legend()\n",
        "\n",
        "        # 4. Component contribution analysis\n",
        "        components = ['Retrieval', 'Reasoning', 'Tools', 'Planning']\n",
        "        contributions = [0.35, 0.4, 0.15, 0.1]  # Simulated weights\n",
        "        colors = plt.cm.Set3(np.linspace(0, 1, len(components)))\n",
        "\n",
        "        wedges, texts, autotexts = axes[1,1].pie(contributions, labels=components, autopct='%1.1f%%',\n",
        "                                                colors=colors, explode=[0.05]*len(components))\n",
        "        axes[1,1].set_title('Component Contribution to Performance')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def _create_learning_analytics(self, df):\n",
        "        \"\"\"Create learning-specific analytics\"\"\"\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=3,\n",
        "            subplot_titles=('Learning Curve', 'Improvement Rate', 'Strategy Evolution',\n",
        "                          'Knowledge Retention', 'Adaptation Speed', 'Learning Efficiency'),\n",
        "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "                   [{\"secondary_y\": False}, {\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        "        )\n",
        "\n",
        "        # 1. Learning curve with confidence bands\n",
        "        cumulative_mean = df['overall_score'].expanding().mean()\n",
        "        cumulative_std = df['overall_score'].expanding().std()\n",
        "\n",
        "        fig.add_trace(go.Scatter(x=list(range(len(df))), y=cumulative_mean,\n",
        "                                mode='lines', name='Learning Curve',\n",
        "                                line=dict(color='blue', width=3)), row=1, col=1)\n",
        "\n",
        "        fig.add_trace(go.Scatter(x=list(range(len(df))), y=cumulative_mean + cumulative_std,\n",
        "                                mode='lines', line=dict(width=0), showlegend=False), row=1, col=1)\n",
        "        fig.add_trace(go.Scatter(x=list(range(len(df))), y=cumulative_mean - cumulative_std,\n",
        "                                mode='lines', fill='tonexty', line=dict(width=0),\n",
        "                                fillcolor='rgba(0,100,200,0.2)', name='Confidence'), row=1, col=1)\n",
        "\n",
        "        # 2. Improvement rate (first derivative)\n",
        "        improvement_rate = df['overall_score'].diff().rolling(window=3).mean()\n",
        "        fig.add_trace(go.Scatter(x=list(range(len(df))), y=improvement_rate,\n",
        "                                mode='lines+markers', name='Improvement Rate',\n",
        "                                line=dict(color='green')), row=1, col=2)\n",
        "\n",
        "        # 3. Strategy evolution heatmap\n",
        "        strategies = ['Basic', 'Semantic', 'Hybrid', 'Adaptive']\n",
        "        time_periods = list(range(0, len(df), max(1, len(df)//4)))\n",
        "        strategy_usage = np.random.rand(len(strategies), len(time_periods))  # Simulated\n",
        "\n",
        "        fig.add_trace(go.Heatmap(z=strategy_usage, x=time_periods, y=strategies,\n",
        "                                colorscale='Viridis', showscale=False), row=1, col=3)\n",
        "\n",
        "        # 4. Knowledge retention (performance on repeated query types)\n",
        "        retention_score = df['overall_score'].rolling(window=5).std()  # Lower std = better retention\n",
        "        fig.add_trace(go.Scatter(x=list(range(len(df))), y=1/retention_score,\n",
        "                                mode='lines', name='Retention Score',\n",
        "                                line=dict(color='purple')), row=2, col=1)\n",
        "\n",
        "        # 5. Adaptation speed (how quickly performance improves after errors)\n",
        "        error_indices = df[df['hallucination_detected']].index\n",
        "        adaptation_scores = []\n",
        "        for idx in error_indices:\n",
        "            post_error = df.iloc[idx+1:idx+4]['overall_score'].mean() if idx+3 < len(df) else df.iloc[idx+1:]['overall_score'].mean()\n",
        "            adaptation_scores.append(post_error)\n",
        "\n",
        "        if adaptation_scores:\n",
        "            fig.add_trace(go.Bar(x=list(range(len(adaptation_scores))), y=adaptation_scores,\n",
        "                                name='Post-Error Recovery', marker_color='orange'), row=2, col=2)\n",
        "\n",
        "        # 6. Learning efficiency (performance improvement per query)\n",
        "        efficiency = cumulative_mean / (np.arange(len(df)) + 1)\n",
        "        fig.add_trace(go.Scatter(x=list(range(len(df))), y=efficiency,\n",
        "                                mode='lines', name='Learning Efficiency',\n",
        "                                line=dict(color='red')), row=2, col=3)\n",
        "\n",
        "        fig.update_layout(height=1000, title_text=\"üß† Learning Analytics Dashboard\")\n",
        "        fig.show()\n",
        "\n",
        "    def _create_error_analysis(self, df):\n",
        "        \"\"\"Create detailed error and failure analysis\"\"\"\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        fig.suptitle('üîç Error Analysis & Failure Patterns', fontsize=16)\n",
        "\n",
        "        # 1. Error frequency over time\n",
        "        error_rate = df['hallucination_detected'].rolling(window=5).mean()\n",
        "        axes[0,0].plot(range(len(df)), error_rate, 'r-', linewidth=2, label='Error Rate')\n",
        "        axes[0,0].fill_between(range(len(df)), error_rate, alpha=0.3, color='red')\n",
        "        axes[0,0].axhline(y=error_rate.mean(), color='black', linestyle='--', label='Average')\n",
        "        axes[0,0].set_title('Error Rate Over Time')\n",
        "        axes[0,0].set_xlabel('Query Number')\n",
        "        axes[0,0].set_ylabel('Error Rate')\n",
        "        axes[0,0].legend()\n",
        "\n",
        "        # 2. Performance vs Error relationship\n",
        "        error_data = df[df['hallucination_detected'] == True]\n",
        "        normal_data = df[df['hallucination_detected'] == False]\n",
        "\n",
        "        axes[0,1].scatter(normal_data['retrieval_accuracy'], normal_data['reasoning_quality'],\n",
        "                         c='blue', alpha=0.6, s=50, label='Normal')\n",
        "        if len(error_data) > 0:\n",
        "            axes[0,1].scatter(error_data['retrieval_accuracy'], error_data['reasoning_quality'],\n",
        "                             c='red', alpha=0.8, s=100, label='Errors', marker='x')\n",
        "        axes[0,1].set_xlabel('Retrieval Accuracy')\n",
        "        axes[0,1].set_ylabel('Reasoning Quality')\n",
        "        axes[0,1].set_title('Error Pattern in Performance Space')\n",
        "        axes[0,1].legend()\n",
        "\n",
        "        # 3. Confidence miscalibration\n",
        "        bins = np.linspace(0, 1, 6)\n",
        "        bin_indices = np.digitize(df['confidence'], bins)\n",
        "        miscalibration = []\n",
        "\n",
        "        for i in range(1, len(bins)):\n",
        "            bin_data = df[bin_indices == i]\n",
        "            if len(bin_data) > 0:\n",
        "                avg_confidence = bin_data['confidence'].mean()\n",
        "                avg_performance = bin_data['overall_score'].mean()\n",
        "                miscalibration.append(abs(avg_confidence - avg_performance))\n",
        "            else:\n",
        "                miscalibration.append(0)\n",
        "\n",
        "        axes[0,2].bar(range(len(miscalibration)), miscalibration, color='orange', alpha=0.7)\n",
        "        axes[0,2].set_title('Confidence Miscalibration by Bin')\n",
        "        axes[0,2].set_xlabel('Confidence Bin')\n",
        "        axes[0,2].set_ylabel('|Confidence - Performance|')\n",
        "\n",
        "        # 4. Error recovery analysis\n",
        "        error_indices = df[df['hallucination_detected']].index\n",
        "        recovery_times = []\n",
        "\n",
        "        for idx in error_indices:\n",
        "            for i in range(1, min(6, len(df) - idx)):\n",
        "                if df.iloc[idx + i]['overall_score'] > df.iloc[idx]['overall_score'] + 0.1:\n",
        "                    recovery_times\n",
        "\n",
        "def demonstrate_rag_agent():\n",
        "    \"\"\"Demonstrate the self-learning RAG agent with multiple queries\"\"\"\n",
        "\n",
        "    print(\"üöÄ Initializing Self-Learning RAG Agent with Z-Reasoning\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Initialize agent\n",
        "    agent = SelfLearningRAGAgent()\n",
        "\n",
        "    # Sample queries to demonstrate different capabilities\n",
        "    demo_queries = [\n",
        "        \"What is artificial intelligence?\",\n",
        "        \"How does machine learning differ from deep learning?\",\n",
        "        \"Explain the concept of reinforcement learning with examples\",\n",
        "        \"Compare natural language processing and computer vision\",\n",
        "        \"What are the advantages and disadvantages of large language models?\",\n",
        "        \"How does RAG improve AI system performance?\",\n",
        "        \"What are the ethical implications of AI development?\",\n",
        "        \"Describe the relationship between AI, ML, and deep learning\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\nüìù Processing Demo Queries...\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i, query in enumerate(demo_queries, 1):\n",
        "        print(f\"\\nQuery {i}: {query}\")\n",
        "        print(\".\" * 30)\n",
        "\n",
        "        result = agent.process_query(query)\n",
        "        results.append(result)\n",
        "\n",
        "        print(f\"Answer: {result['answer'][:100]}...\")\n",
        "        print(f\"Confidence: {result['confidence']:.2f}\")\n",
        "        print(f\"Overall Score: {result['critique']['overall_score']:.2f}\")\n",
        "\n",
        "        if result['improvements']:\n",
        "            print(f\"Suggestions: {', '.join(result['improvements'])}\")\n",
        "\n",
        "    print(\"\\nüìä Generating Performance Analytics...\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Get analytics\n",
        "    analytics = agent.get_performance_analytics()\n",
        "\n",
        "    print(\"\\nüìà Performance Summary:\")\n",
        "    print(f\"Total Queries Processed: {analytics['summary_stats']['total_queries']}\")\n",
        "    print(f\"Average Overall Score: {analytics['summary_stats']['avg_overall_score']:.3f}\")\n",
        "    print(f\"Average Retrieval Accuracy: {analytics['summary_stats']['avg_retrieval_accuracy']:.3f}\")\n",
        "    print(f\"Average Reasoning Quality: {analytics['summary_stats']['avg_reasoning_quality']:.3f}\")\n",
        "    print(f\"Hallucination Rate: {analytics['summary_stats']['hallucination_rate']:.1%}\")\n",
        "\n",
        "    print(\"\\nüìà Learning Trends:\")\n",
        "    print(f\"Score Improvement Trend: {analytics['trends']['score_improvement']:.3f}\")\n",
        "    print(f\"Retrieval Improvement Trend: {analytics['trends']['retrieval_improvement']:.3f}\")\n",
        "    print(f\"Reasoning Improvement Trend: {analytics['trends']['reasoning_improvement']:.3f}\")\n",
        "\n",
        "    # Generate visualizations\n",
        "    print(\"\\nüìä Generating Visualizations...\")\n",
        "    agent.visualize_performance()\n",
        "\n",
        "    return agent, results, analytics\n",
        "\n",
        "def analyze_system_architecture():\n",
        "    \"\"\"Create detailed system architecture visualization\"\"\"\n",
        "\n",
        "    print(\"\\nüèóÔ∏è RAG Agent System Architecture Analysis\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Create architecture flow diagram\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Define component positions\n",
        "    components = {\n",
        "        'User Query': (1, 5),\n",
        "        'Planner Agent': (3, 5),\n",
        "        'Retriever': (5, 6),\n",
        "        'Vector DB': (7, 7),\n",
        "        'Long-Term Memory': (7, 5),\n",
        "        'Reasoning Agent (X)': (5, 4),\n",
        "        'Tool Agent (Y)': (5, 2),\n",
        "        'Critic Agent (Z)': (3, 2),\n",
        "        'Final Answer': (1, 2)\n",
        "    }\n",
        "\n",
        "    # Add nodes\n",
        "    for comp, (x, y) in components.items():\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=[x], y=[y],\n",
        "            mode='markers+text',\n",
        "            text=[comp],\n",
        "            textposition='middle center',\n",
        "            marker=dict(size=80, color='lightblue'),\n",
        "            showlegend=False\n",
        "        ))\n",
        "\n",
        "    # Add connections (arrows would be more complex in plotly)\n",
        "    connections = [\n",
        "        ('User Query', 'Planner Agent'),\n",
        "        ('Planner Agent', 'Retriever'),\n",
        "        ('Retriever', 'Vector DB'),\n",
        "        ('Retriever', 'Long-Term Memory'),\n",
        "        ('Planner Agent', 'Reasoning Agent (X)'),\n",
        "        ('Planner Agent', 'Tool Agent (Y)'),\n",
        "        ('Reasoning Agent (X)', 'Critic Agent (Z)'),\n",
        "        ('Tool Agent (Y)', 'Critic Agent (Z)'),\n",
        "        ('Critic Agent (Z)', 'Long-Term Memory'),\n",
        "        ('Critic Agent (Z)', 'Final Answer')\n",
        "    ]\n",
        "\n",
        "    for start, end in connections:\n",
        "        start_pos = components[start]\n",
        "        end_pos = components[end]\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=[start_pos[0], end_pos[0]],\n",
        "            y=[start_pos[1], end_pos[1]],\n",
        "            mode='lines',\n",
        "            line=dict(color='gray', width=2),\n",
        "            showlegend=False\n",
        "        ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=\"Self-Learning RAG Agent Architecture\",\n",
        "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "        height=600,\n",
        "        width=1000\n",
        "    )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "    # Component analysis\n",
        "    print(\"\\nüîç Component Analysis:\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    component_analysis = {\n",
        "        'Planner Agent': {\n",
        "            'Purpose': 'Query decomposition and execution planning',\n",
        "            'Key Features': ['Query classification', 'Sub-query generation', 'Strategy selection'],\n",
        "            'Z-Reasoning Integration': 'Adapts planning based on past performance'\n",
        "        },\n",
        "        'Enhanced Retriever': {\n",
        "            'Purpose': 'Adaptive document retrieval',\n",
        "            'Key Features': ['Multiple retrieval strategies', 'Query expansion', 'Relevance scoring'],\n",
        "            'Z-Reasoning Integration': 'Adjusts retrieval parameters based on critic feedback'\n",
        "        },\n",
        "        'Reasoning Agent (X)': {\n",
        "            'Purpose': 'Primary reasoning and answer generation',\n",
        "            'Key Features': ['Chain-of-thought reasoning', 'Confidence estimation', 'Step-by-step analysis'],\n",
        "            'Z-Reasoning Integration': 'Reasoning patterns evolve based on critique'\n",
        "        },\n",
        "        'Tool Agent (Y)': {\n",
        "            'Purpose': 'External tool integration',\n",
        "            'Key Features': ['Web search', 'Database queries', 'Fact checking', 'Calculations'],\n",
        "            'Z-Reasoning Integration': 'Tool selection optimized by performance history'\n",
        "        },\n",
        "        'Critic Agent (Z)': {\n",
        "            'Purpose': 'Self-assessment and continuous learning',\n",
        "            'Key Features': ['Quality assessment', 'Hallucination detection', 'Improvement suggestions'],\n",
        "            'Z-Reasoning Integration': 'Core of the self-learning mechanism'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for component, details in component_analysis.items():\n",
        "        print(f\"\\n{component}:\")\n",
        "        print(f\"  Purpose: {details['Purpose']}\")\n",
        "        print(f\"  Features: {', '.join(details['Key Features'])}\")\n",
        "        print(f\"  Z-Integration: {details['Z-Reasoning Integration']}\")\n",
        "\n",
        "def create_learning_mechanism_analysis():\n",
        "    \"\"\"Analyze the Z-reasoning learning mechanism\"\"\"\n",
        "\n",
        "    print(\"\\nüß† Z-Reasoning Learning Mechanism Analysis\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Create learning loop visualization\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        subplot_titles=('Learning Feedback Loop', 'Performance Metrics Evolution',\n",
        "                       'Strategy Adaptation Process', 'Memory Utilization'),\n",
        "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "               [{\"secondary_y\": False}, {\"type\": \"pie\"}]]\n",
        "    )\n",
        "\n",
        "    # Simulate learning data\n",
        "    iterations = list(range(1, 21))\n",
        "    base_performance = 0.5\n",
        "    improvement_rate = 0.02\n",
        "    noise = np.random.normal(0, 0.05, 20)\n",
        "\n",
        "    performance_scores = [base_performance + i * improvement_rate + noise[i-1] for i in iterations]\n",
        "    retrieval_accuracy = [0.4 + i * 0.025 + np.random.normal(0, 0.03) for i in iterations]\n",
        "    reasoning_quality = [0.45 + i * 0.02 + np.random.normal(0, 0.04) for i in iterations]\n",
        "\n",
        "    # Plot 1: Learning feedback loop\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=iterations, y=performance_scores, mode='lines+markers',\n",
        "                  name='Overall Performance', line=dict(color='blue')),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "    # Plot 2: Component evolution\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=iterations, y=retrieval_accuracy, mode='lines',\n",
        "                  name='Retrieval', line=dict(color='green')),\n",
        "        row=1, col=2\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=iterations, y=reasoning_quality, mode='lines',\n",
        "                  name='Reasoning', line=dict(color='orange')),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "    # Plot 3: Strategy adaptation\n",
        "    strategies = ['Basic', 'Semantic', 'Hybrid', 'Adaptive']\n",
        "    usage_counts = [5, 8, 12, 15]\n",
        "    success_rates = [0.6, 0.7, 0.8, 0.85]\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Bar(x=strategies, y=success_rates, name='Success Rate',\n",
        "               marker_color='lightcoral'),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "    # Plot 4: Memory utilization\n",
        "    memory_categories = ['Interactions', 'Strategies', 'Patterns', 'Feedback']\n",
        "    memory_sizes = [150, 25, 40, 80]\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Pie(labels=memory_categories, values=memory_sizes, name='Memory'),\n",
        "        row=2, col=2\n",
        "    )\n",
        "\n",
        "    fig.update_layout(height=800, title_text=\"Z-Reasoning Learning Mechanism\")\n",
        "    fig.show()\n",
        "\n",
        "    # Learning principles analysis\n",
        "    print(\"\\nüìö Learning Principles:\")\n",
        "    print(\"-\" * 25)\n",
        "\n",
        "    learning_principles = {\n",
        "        'Self-Consistency Checking': {\n",
        "            'Description': 'Agent compares outputs with retrieved context',\n",
        "            'Implementation': 'Hallucination detection, factual grounding checks',\n",
        "            'Benefit': 'Reduces false information generation'\n",
        "        },\n",
        "        'Performance Reflection': {\n",
        "            'Description': 'Continuous assessment of reasoning quality',\n",
        "            'Implementation': 'Critic agent evaluation, confidence scoring',\n",
        "            'Benefit': 'Identifies weaknesses and improvement areas'\n",
        "        },\n",
        "        'Strategy Evolution': {\n",
        "            'Description': 'Adaptive selection of retrieval and reasoning approaches',\n",
        "            'Implementation': 'Performance-based strategy ranking and selection',\n",
        "            'Benefit': 'Optimizes approach for different query types'\n",
        "        },\n",
        "        'Memory Integration': {\n",
        "            'Description': 'Long-term storage and retrieval of past experiences',\n",
        "            'Implementation': 'SQLite database with structured memory storage',\n",
        "            'Benefit': 'Enables learning from past successes and failures'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for principle, details in learning_principles.items():\n",
        "        print(f\"\\n{principle}:\")\n",
        "        print(f\"  ‚Ä¢ {details['Description']}\")\n",
        "        print(f\"  ‚Ä¢ Implementation: {details['Implementation']}\")\n",
        "        print(f\"  ‚Ä¢ Benefit: {details['Benefit']}\")\n",
        "\n",
        "# Main execution function\n",
        "def main():\n",
        "    \"\"\"Main execution function for Colab\"\"\"\n",
        "    print(\"ü§ñ Self-Learning RAG Agent with Z-Reasoning\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"This implementation demonstrates a comprehensive RAG system with:\")\n",
        "    print(\"‚Ä¢ Multi-agent architecture (Planner, Retriever, Reasoner, Tool, Critic)\")\n",
        "    print(\"‚Ä¢ Self-assessment and learning capabilities\")\n",
        "    print(\"‚Ä¢ Z-reasoning for continuous improvement\")\n",
        "    print(\"‚Ä¢ Comprehensive performance analytics\")\n",
        "    print(\"‚Ä¢ Detailed visualizations and analysis\")\n",
        "\n",
        "    # Run demonstrations\n",
        "    try:\n",
        "        # System architecture analysis\n",
        "        analyze_system_architecture()\n",
        "\n",
        "        # Learning mechanism analysis\n",
        "        create_learning_mechanism_analysis()\n",
        "\n",
        "        # Main demonstration\n",
        "        agent, results, analytics = demonstrate_rag_agent()\n",
        "\n",
        "        print(\"\\n‚úÖ Demonstration completed successfully!\")\n",
        "        print(f\"Processed {len(results)} queries with learning-based improvement\")\n",
        "\n",
        "        return agent, results, analytics\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during execution: {str(e)}\")\n",
        "        return None, None, None\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    agent, results, analytics = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ea54cb8b198840be8fca420c33821ffc",
            "5239a0fc2eba4b7c9b7a2f57d3f03c08",
            "26916008cc844a0787cb0ad6d7258adb",
            "4af1cebb7b2e4251a51140e7323e1a1b",
            "ba4abd901b89486b93fb236f35565431",
            "e33862da87bb48b4b683934669d4f33f",
            "0244938ae210412da2c65b6d3d863c50",
            "38b006920f61424284ec6622c3dce0ee",
            "d249e3698fd342a9becddf7bd7937518",
            "01fe42c94f634903a92fb4c236c7ef1a",
            "7f91f80250884f689f733e17d8cd5c3c",
            "43ac23d5c9a54b3b978ff9c38b33bee9",
            "97780847546a42ff82041c689c7ecc81",
            "516e668d144f4100922d78dc5c205969",
            "58bfd2dd869944dcaa900fd06984d2ab",
            "1560196189d74f90bf413e7ad2b685ad",
            "b12056d2d32c48f08f58ef0be5ae5fa2",
            "82132d79de1f410fab75094c974c05a4",
            "756ea341e01146838edb9d0f46e69a00",
            "a786a342f29f44f6a8d91699d0c7784a",
            "df6a74d7e2924d7eba5a98b5f4faa687",
            "16a3c590ef4d4cee84e795c7a1d2309c",
            "953a361a9565416b94c7a040fd89128a",
            "15c625230f454e63886b95d3f9ddd97f",
            "3f97a71472ab41478f4dbf9d343d637f",
            "8f8d433de60441acb8a8c555cfc9bab2",
            "16677d98137c4f1fa4abd7283d3b3920",
            "7d027a41dab440de83e3cb14b4742a2a",
            "96a5ba53529c4a688af02ab741ece8c9",
            "913204cf450f44d09a3ac7ddd6328f9d",
            "a7f326f7ecf1447f8267b70eac51a438",
            "7d62c93a5a1d453db2174351390a2a46",
            "c7a62490662a467bb715b1d223241822",
            "456abcb2b26f492c8fefa47cf3f860e5",
            "39adfe1d7a6f436590bc33900b01eed7",
            "347063e287eb4d8e90d5d9d4a8a61f2f",
            "447d4e21f91f4944bd7f1a5af5725297",
            "1fc13972ca8c44b38eb070a218ac578e",
            "d4f8d3c3bc4a46deb0fc02ce91aaa979",
            "86a7e8a363304de4bef0da53897505f0",
            "1d65f1d479b0473bab09d112ce2ee0dd",
            "d1d4212a185f4a798403f6942a1dec4e",
            "c4519d759e8e43fa855e8a9d72d56208",
            "7b41f44b8ea84691b000ced84ccdf245",
            "4a8d59a6b2a24bdca4828689774e9f64",
            "f5a9827fcfc14976be571af4dd7b9015",
            "6bd58b489dc7463ea9a2157934b91876",
            "36f86a2886a948029ac8623a058036d4",
            "62b02aff500643f4b4e31a95712fa49e",
            "d2f9787a9b2449459727a8de48b9ecef",
            "2568aabbd93f4487bcf43f63dde247ad",
            "529a6c261b184ef48ad6a881062b7f89",
            "90b095ab40c747f581ea7dfe37627c3d",
            "8502fba740d4464fa8c5fd2dd7e6db4b",
            "52ee1471c68a4a8d8d35437971f1a0df",
            "96d4af31e7dc4c6385272aa21ad593ac",
            "d712cef30b8a4934b287775fb369bb49",
            "7b86b370cef6402b893dfaa32e341e7f",
            "58c5fbf898694c14a1851bbe7f8b4033",
            "952b4917aca841a2921e993b669be24a",
            "5218f56d57a64c198565369b9942aae5",
            "3bf76a3dda0b457189d3a56f619381aa",
            "9e4adf18fe7b4c6f908931d488ae5c28",
            "0734b0b5ba064ad8996116eb6f1b946f",
            "8545e119584d437392ca3a5495f1b159",
            "ef7b496e880143e185a82a9dcaedf495",
            "f67c13fae3c04cfea27fc516bc25063e",
            "9b6d9784711b465f88dfe1df7282c1df",
            "aada62f92fe048e986000ef9b1d5b884",
            "725e1a6247094da5bc1a09f4b9e8a821",
            "6e34d06daea34c41aa6248e394de6466",
            "91eba46556414c16bb3e90a1f0f5ef5c",
            "2ab37ba86a3143d2b6ef09977cae176b",
            "b8d332b7cac8445fbc5f6223c1c3c196",
            "319f52cfbd334795ad4bc327ef7a937a",
            "f56fc71305004276baf5eba218a21d26",
            "dddee5f09c824846ac59eb21ebde5b90",
            "2de7e4ba53a441bbb5951a8f57110013",
            "5bdbc514471845499a917b8e80025aa9",
            "1de42db3c44540d0917c07b5a7bc58da",
            "86f5db933cc946de9efe8842e1899dee",
            "37b19703131c40989eeb34d8344a88bc",
            "6e70fdb8a4e2421fa01e218e167ad4ed",
            "9fa2afe3983449aba5f0b74fb1760dec",
            "b4a4caf2d2b941d8ba8e62ca300f5f99",
            "e0278cbd68514c9f84943311d904a1e4",
            "cd444cb6d0934a119e13814a1ff876ed",
            "1aac2420fa40473c96a727739deedae5",
            "4f9e688e8c2243d0b217a2c742f16105",
            "c9c8772d0a4548b791431dd5dda8d5f5",
            "09c54b149bab425182658ebef78aa923",
            "a5a3f8eaff7247cf9bcb8c59e32f0121",
            "1565f07aab3140f7a73f97ff6d46768e",
            "9ff05b9b87d0490db0d25c38ed757f8c",
            "13f063a886104666ad1a86553fa39834",
            "752944e532bf4df7b2966257b09df427",
            "1e9119707c204dcb8427423212ad0fac",
            "4f273c24ff3a4b3b95cd7b33b1009867",
            "36b7a56108c34053a43fa4c0a79c19d4",
            "f1ba6e43784944f5a3aa6b42b1236f99",
            "e5d5eb4b5f3d4d39a2ced69c2f8e066f",
            "a7cc8b5e1aaf4b439db9ef41186fcc4b",
            "110b1add08eb4806b1dad191c241eb7d",
            "a5ed65046c34482bb709501a3d20ca49",
            "45bd54ef02664c00a4a62c4ca92c732a",
            "dd62094120574852acf27613ad2387e2",
            "120174d8fce8443c95826feac804ecd3",
            "f93e14fac90642aa83fee53fd8e856be",
            "0fe85a40a9cd41a4b0b84c477bf5a071",
            "17a82180238f4c1e91326e31469d7f4d",
            "94e44da422784514a23b20e3f5f414f8",
            "49447cf1c3754c11b0f99264bc80184a",
            "fc645f24ae904d698d940deb66f2fc0d",
            "e7deb2f5d9884f2f95a10b56e4a2414c",
            "7689b5357d8840779107953ce5b819bc",
            "467375581ffc4f5e8c3c4ce4ff5c241d",
            "5daca735435e4ea38ae7fd17c220386f",
            "ec2c9e94e810432a8f1835ba02b43fd3",
            "b868e40710eb419a9f1ad3e13b9a6165",
            "c49ee61ce27d47d7ac8721ed1b26366d",
            "3e3fe7764b7d4d71930bed38e23bd23f"
          ]
        },
        "id": "9qVPPhWJEB3G",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1757717614154,
          "user_tz": 420,
          "elapsed": 19512,
          "user": {
            "displayName": "Samira Samrose",
            "userId": "18055169407059979499"
          }
        },
        "outputId": "abf96b3b-b4a5-4cde-df02-5ee13a53aa79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üé® VISUALIZATION SHOWCASE\n",
            "==================================================\n",
            "This demonstration includes:\n",
            "\n",
            "üìà Interactive Plotly Charts:\n",
            "   ‚Ä¢ Multi-metric timeline with dual axes\n",
            "   ‚Ä¢ Radar charts for component performance\n",
            "   ‚Ä¢ Box plots for distribution analysis\n",
            "   ‚Ä¢ Learning velocity tracking\n",
            "   ‚Ä¢ Confidence vs accuracy scatter plots\n",
            "   ‚Ä¢ Strategy effectiveness over time\n",
            "\n",
            "üìà Statistical Analysis:\n",
            "   ‚Ä¢ Distribution analysis with KDE\n",
            "   ‚Ä¢ Correlation matrices with annotations\n",
            "   ‚Ä¢ Performance quartile analysis\n",
            "   ‚Ä¢ Confidence interval bands\n",
            "   ‚Ä¢ Hexbin plots for complexity analysis\n",
            "   ‚Ä¢ Outlier detection visualizations\n",
            "\n",
            "üìà Temporal Analysis:\n",
            "   ‚Ä¢ STL decomposition simulation\n",
            "   ‚Ä¢ Seasonal pattern detection\n",
            "   ‚Ä¢ Multiple moving averages comparison\n",
            "   ‚Ä¢ Volatility analysis with rolling statistics\n",
            "\n",
            "üìà Learning Analytics:\n",
            "   ‚Ä¢ Learning curves with confidence bands\n",
            "   ‚Ä¢ Improvement rate tracking\n",
            "   ‚Ä¢ Strategy evolution heatmaps\n",
            "   ‚Ä¢ Knowledge retention analysis\n",
            "   ‚Ä¢ Adaptation speed measurements\n",
            "   ‚Ä¢ Learning efficiency metrics\n",
            "\n",
            "üìà Error Analysis:\n",
            "   ‚Ä¢ Error frequency over time\n",
            "   ‚Ä¢ Performance vs error relationship plots\n",
            "   ‚Ä¢ Confidence miscalibration analysis\n",
            "   ‚Ä¢ Error recovery time distribution\n",
            "   ‚Ä¢ Performance degradation patterns\n",
            "   ‚Ä¢ Error clustering analysis\n",
            "\n",
            "üìà 3D Visualizations:\n",
            "   ‚Ä¢ 3D performance space clustering\n",
            "   ‚Ä¢ 3D learning trajectory over time\n",
            "   ‚Ä¢ Multi-dimensional scatter plots\n",
            "   ‚Ä¢ Interactive 3D surface plots\n",
            "\n",
            "üìà Advanced Analytics:\n",
            "   ‚Ä¢ Network analysis of component interactions\n",
            "   ‚Ä¢ Machine learning clustering analysis\n",
            "   ‚Ä¢ Time series forecasting with confidence bands\n",
            "   ‚Ä¢ Anomaly detection (statistical + ML)\n",
            "   ‚Ä¢ Causal relationship analysis\n",
            "   ‚Ä¢ Performance prediction models\n",
            "\n",
            "üéØ Key Features:\n",
            "   ‚Ä¢ Real-time performance tracking\n",
            "   ‚Ä¢ Multi-dimensional analysis\n",
            "   ‚Ä¢ Predictive analytics\n",
            "   ‚Ä¢ Interactive visualizations\n",
            "   ‚Ä¢ Automated insights generation\n",
            "   ‚Ä¢ Comprehensive reporting\n",
            "ü§ñ Self-Learning RAG Agent with Z-Reasoning\n",
            "============================================================\n",
            "This implementation demonstrates a comprehensive RAG system with:\n",
            "‚Ä¢ Multi-agent architecture (Planner, Retriever, Reasoner, Tool, Critic)\n",
            "‚Ä¢ Self-assessment and learning capabilities\n",
            "‚Ä¢ Z-reasoning for continuous improvement\n",
            "‚Ä¢ Advanced analytics with 15+ visualization types\n",
            "‚Ä¢ Machine learning performance analysis\n",
            "‚Ä¢ Comprehensive reporting system\n",
            "\n",
            "üèóÔ∏è RAG Agent System Architecture Analysis\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"3d17bcc5-9fa2-4cca-b72e-4745f9456a6b\" class=\"plotly-graph-div\" style=\"height:600px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"3d17bcc5-9fa2-4cca-b72e-4745f9456a6b\")) {                    Plotly.newPlot(                        \"3d17bcc5-9fa2-4cca-b72e-4745f9456a6b\",                        [{\"marker\":{\"color\":\"lightblue\",\"size\":80},\"mode\":\"markers+text\",\"showlegend\":false,\"text\":[\"User Query\"],\"textposition\":\"middle center\",\"x\":[1],\"y\":[5],\"type\":\"scatter\"},{\"marker\":{\"color\":\"lightblue\",\"size\":80},\"mode\":\"markers+text\",\"showlegend\":false,\"text\":[\"Planner Agent\"],\"textposition\":\"middle center\",\"x\":[3],\"y\":[5],\"type\":\"scatter\"},{\"marker\":{\"color\":\"lightblue\",\"size\":80},\"mode\":\"markers+text\",\"showlegend\":false,\"text\":[\"Retriever\"],\"textposition\":\"middle center\",\"x\":[5],\"y\":[6],\"type\":\"scatter\"},{\"marker\":{\"color\":\"lightblue\",\"size\":80},\"mode\":\"markers+text\",\"showlegend\":false,\"text\":[\"Vector DB\"],\"textposition\":\"middle center\",\"x\":[7],\"y\":[7],\"type\":\"scatter\"},{\"marker\":{\"color\":\"lightblue\",\"size\":80},\"mode\":\"markers+text\",\"showlegend\":false,\"text\":[\"Long-Term Memory\"],\"textposition\":\"middle center\",\"x\":[7],\"y\":[5],\"type\":\"scatter\"},{\"marker\":{\"color\":\"lightblue\",\"size\":80},\"mode\":\"markers+text\",\"showlegend\":false,\"text\":[\"Reasoning Agent (X)\"],\"textposition\":\"middle center\",\"x\":[5],\"y\":[4],\"type\":\"scatter\"},{\"marker\":{\"color\":\"lightblue\",\"size\":80},\"mode\":\"markers+text\",\"showlegend\":false,\"text\":[\"Tool Agent (Y)\"],\"textposition\":\"middle center\",\"x\":[5],\"y\":[2],\"type\":\"scatter\"},{\"marker\":{\"color\":\"lightblue\",\"size\":80},\"mode\":\"markers+text\",\"showlegend\":false,\"text\":[\"Critic Agent (Z)\"],\"textposition\":\"middle center\",\"x\":[3],\"y\":[2],\"type\":\"scatter\"},{\"marker\":{\"color\":\"lightblue\",\"size\":80},\"mode\":\"markers+text\",\"showlegend\":false,\"text\":[\"Final Answer\"],\"textposition\":\"middle center\",\"x\":[1],\"y\":[2],\"type\":\"scatter\"},{\"line\":{\"color\":\"gray\",\"width\":2},\"mode\":\"lines\",\"showlegend\":false,\"x\":[1,3],\"y\":[5,5],\"type\":\"scatter\"},{\"line\":{\"color\":\"gray\",\"width\":2},\"mode\":\"lines\",\"showlegend\":false,\"x\":[3,5],\"y\":[5,6],\"type\":\"scatter\"},{\"line\":{\"color\":\"gray\",\"width\":2},\"mode\":\"lines\",\"showlegend\":false,\"x\":[5,7],\"y\":[6,7],\"type\":\"scatter\"},{\"line\":{\"color\":\"gray\",\"width\":2},\"mode\":\"lines\",\"showlegend\":false,\"x\":[5,7],\"y\":[6,5],\"type\":\"scatter\"},{\"line\":{\"color\":\"gray\",\"width\":2},\"mode\":\"lines\",\"showlegend\":false,\"x\":[3,5],\"y\":[5,4],\"type\":\"scatter\"},{\"line\":{\"color\":\"gray\",\"width\":2},\"mode\":\"lines\",\"showlegend\":false,\"x\":[3,5],\"y\":[5,2],\"type\":\"scatter\"},{\"line\":{\"color\":\"gray\",\"width\":2},\"mode\":\"lines\",\"showlegend\":false,\"x\":[5,3],\"y\":[4,2],\"type\":\"scatter\"},{\"line\":{\"color\":\"gray\",\"width\":2},\"mode\":\"lines\",\"showlegend\":false,\"x\":[5,3],\"y\":[2,2],\"type\":\"scatter\"},{\"line\":{\"color\":\"gray\",\"width\":2},\"mode\":\"lines\",\"showlegend\":false,\"x\":[3,7],\"y\":[2,5],\"type\":\"scatter\"},{\"line\":{\"color\":\"gray\",\"width\":2},\"mode\":\"lines\",\"showlegend\":false,\"x\":[3,1],\"y\":[2,2],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"showgrid\":false,\"zeroline\":false,\"showticklabels\":false},\"yaxis\":{\"showgrid\":false,\"zeroline\":false,\"showticklabels\":false},\"title\":{\"text\":\"Self-Learning RAG Agent Architecture\"},\"height\":600,\"width\":1000},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('3d17bcc5-9fa2-4cca-b72e-4745f9456a6b');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Component Analysis:\n",
            "------------------------------\n",
            "\n",
            "Planner Agent:\n",
            "  Purpose: Query decomposition and execution planning\n",
            "  Features: Query classification, Sub-query generation, Strategy selection\n",
            "  Z-Integration: Adapts planning based on past performance\n",
            "\n",
            "Enhanced Retriever:\n",
            "  Purpose: Adaptive document retrieval\n",
            "  Features: Multiple retrieval strategies, Query expansion, Relevance scoring\n",
            "  Z-Integration: Adjusts retrieval parameters based on critic feedback\n",
            "\n",
            "Reasoning Agent (X):\n",
            "  Purpose: Primary reasoning and answer generation\n",
            "  Features: Chain-of-thought reasoning, Confidence estimation, Step-by-step analysis\n",
            "  Z-Integration: Reasoning patterns evolve based on critique\n",
            "\n",
            "Tool Agent (Y):\n",
            "  Purpose: External tool integration\n",
            "  Features: Web search, Database queries, Fact checking, Calculations\n",
            "  Z-Integration: Tool selection optimized by performance history\n",
            "\n",
            "Critic Agent (Z):\n",
            "  Purpose: Self-assessment and continuous learning\n",
            "  Features: Quality assessment, Hallucination detection, Improvement suggestions\n",
            "  Z-Integration: Core of the self-learning mechanism\n",
            "\n",
            "üß† Z-Reasoning Learning Mechanism Analysis\n",
            "==================================================\n",
            "‚ùå Error during execution: Trace type 'pie' is not compatible with subplot type 'xy'\n",
            "at grid position (2, 2)\n",
            "\n",
            "See the docstring for the specs argument to plotly.subplots.make_subplots\n",
            "for more information on subplot types\n",
            "\n",
            "üéØ Want to see specific visualization types?\n",
            "Uncomment the following lines to run specific demos:\n",
            "# demonstrate_specific_visualization('interactive')\n",
            "# demonstrate_specific_visualization('statistical')\n",
            "# demonstrate_specific_visualization('learning')\n",
            "# create_custom_visualization_examples()\n",
            "ü§ñ Self-Learning RAG Agent with Z-Reasoning\n",
            "============================================================\n",
            "This implementation demonstrates a comprehensive RAG system with:\n",
            "‚Ä¢ Multi-agent architecture (Planner, Retriever, Reasoner, Tool, Critic)\n",
            "‚Ä¢ Self-assessment and learning capabilities\n",
            "‚Ä¢ Z-reasoning for continuous improvement\n",
            "‚Ä¢ Comprehensive performance analytics\n",
            "‚Ä¢ Detailed visualizations and analysis\n",
            "\n",
            "üèóÔ∏è RAG Agent System Architecture Analysis\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-1362197085.py\", line 20, in main\n",
            "    create_learning_mechanism_analysis()\n",
            "  File \"/tmp/ipython-input-1438832265.py\", line 2018, in create_learning_mechanism_analysis\n",
            "    fig.add_trace(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/plotly/graph_objs/_figure.py\", line 917, in add_trace\n",
            "    return super(Figure, self).add_trace(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/plotly/basedatatypes.py\", line 2106, in add_trace\n",
            "    return self.add_traces(\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/plotly/graph_objs/_figure.py\", line 997, in add_traces\n",
            "    return super(Figure, self).add_traces(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/plotly/basedatatypes.py\", line 2236, in add_traces\n",
            "    self._set_trace_grid_position(trace, row, col, secondary_y)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/plotly/basedatatypes.py\", line 2328, in _set_trace_grid_position\n",
            "    return _set_trace_grid_reference(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/plotly/_subplots.py\", line 1415, in _set_trace_grid_reference\n",
            "    raise ValueError(\n",
            "ValueError: Trace type 'pie' is not compatible with subplot type 'xy'\n",
            "at grid position (2, 2)\n",
            "\n",
            "See the docstring for the specs argument to plotly.subplots.make_subplots\n",
            "for more information on subplot types\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"5c6e3d25-a8ff-4245-ad59-49d5f9af1bd7\" class=\"plotly-graph-div\" style=\"height:600px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5c6e3d25-a8ff-4245-ad59-49d5f9af1bd7\")) {                    Plotly.newPlot(                        \"5c6e3d25-a8ff-4245-ad59-49d5f9af1bd7\",                        [{\"marker\":{\"color\":\"lightblue\",\"size\":80},\"mode\":\"markers+text\",\"showlegend\":false,\"text\":[\"User Query\"],\"textposition\":\"middle center\",\"x\":[1],\"y\":[5],\"type\":\"scatter\"},{\"marker\":{\"color\":\"lightblue\",\"size\":80},\"mode\":\"markers+text\",\"showlegend\":false,\"text\":[\"Planner Agent\"],\"textposition\":\"middle center\",\"x\":[3],\"y\":[5],\"type\":\"scatter\"},{\"marker\":{\"color\":\"lightblue\",\"size\":80},\"mode\":\"markers+text\",\"showlegend\":false,\"text\":[\"Retriever\"],\"textposition\":\"middle center\",\"x\":[5],\"y\":[6],\"type\":\"scatter\"},{\"marker\":{\"color\":\"lightblue\",\"size\":80},\"mode\":\"markers+text\",\"showlegend\":false,\"text\":[\"Vector DB\"],\"textposition\":\"middle center\",\"x\":[7],\"y\":[7],\"type\":\"scatter\"},{\"marker\":{\"color\":\"lightblue\",\"size\":80},\"mode\":\"markers+text\",\"showlegend\":false,\"text\":[\"Long-Term Memory\"],\"textposition\":\"middle center\",\"x\":[7],\"y\":[5],\"type\":\"scatter\"},{\"marker\":{\"color\":\"lightblue\",\"size\":80},\"mode\":\"markers+text\",\"showlegend\":false,\"text\":[\"Reasoning Agent (X)\"],\"textposition\":\"middle center\",\"x\":[5],\"y\":[4],\"type\":\"scatter\"},{\"marker\":{\"color\":\"lightblue\",\"size\":80},\"mode\":\"markers+text\",\"showlegend\":false,\"text\":[\"Tool Agent (Y)\"],\"textposition\":\"middle center\",\"x\":[5],\"y\":[2],\"type\":\"scatter\"},{\"marker\":{\"color\":\"lightblue\",\"size\":80},\"mode\":\"markers+text\",\"showlegend\":false,\"text\":[\"Critic Agent (Z)\"],\"textposition\":\"middle center\",\"x\":[3],\"y\":[2],\"type\":\"scatter\"},{\"marker\":{\"color\":\"lightblue\",\"size\":80},\"mode\":\"markers+text\",\"showlegend\":false,\"text\":[\"Final Answer\"],\"textposition\":\"middle center\",\"x\":[1],\"y\":[2],\"type\":\"scatter\"},{\"line\":{\"color\":\"gray\",\"width\":2},\"mode\":\"lines\",\"showlegend\":false,\"x\":[1,3],\"y\":[5,5],\"type\":\"scatter\"},{\"line\":{\"color\":\"gray\",\"width\":2},\"mode\":\"lines\",\"showlegend\":false,\"x\":[3,5],\"y\":[5,6],\"type\":\"scatter\"},{\"line\":{\"color\":\"gray\",\"width\":2},\"mode\":\"lines\",\"showlegend\":false,\"x\":[5,7],\"y\":[6,7],\"type\":\"scatter\"},{\"line\":{\"color\":\"gray\",\"width\":2},\"mode\":\"lines\",\"showlegend\":false,\"x\":[5,7],\"y\":[6,5],\"type\":\"scatter\"},{\"line\":{\"color\":\"gray\",\"width\":2},\"mode\":\"lines\",\"showlegend\":false,\"x\":[3,5],\"y\":[5,4],\"type\":\"scatter\"},{\"line\":{\"color\":\"gray\",\"width\":2},\"mode\":\"lines\",\"showlegend\":false,\"x\":[3,5],\"y\":[5,2],\"type\":\"scatter\"},{\"line\":{\"color\":\"gray\",\"width\":2},\"mode\":\"lines\",\"showlegend\":false,\"x\":[5,3],\"y\":[4,2],\"type\":\"scatter\"},{\"line\":{\"color\":\"gray\",\"width\":2},\"mode\":\"lines\",\"showlegend\":false,\"x\":[5,3],\"y\":[2,2],\"type\":\"scatter\"},{\"line\":{\"color\":\"gray\",\"width\":2},\"mode\":\"lines\",\"showlegend\":false,\"x\":[3,7],\"y\":[2,5],\"type\":\"scatter\"},{\"line\":{\"color\":\"gray\",\"width\":2},\"mode\":\"lines\",\"showlegend\":false,\"x\":[3,1],\"y\":[2,2],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"showgrid\":false,\"zeroline\":false,\"showticklabels\":false},\"yaxis\":{\"showgrid\":false,\"zeroline\":false,\"showticklabels\":false},\"title\":{\"text\":\"Self-Learning RAG Agent Architecture\"},\"height\":600,\"width\":1000},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('5c6e3d25-a8ff-4245-ad59-49d5f9af1bd7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Component Analysis:\n",
            "------------------------------\n",
            "\n",
            "Planner Agent:\n",
            "  Purpose: Query decomposition and execution planning\n",
            "  Features: Query classification, Sub-query generation, Strategy selection\n",
            "  Z-Integration: Adapts planning based on past performance\n",
            "\n",
            "Enhanced Retriever:\n",
            "  Purpose: Adaptive document retrieval\n",
            "  Features: Multiple retrieval strategies, Query expansion, Relevance scoring\n",
            "  Z-Integration: Adjusts retrieval parameters based on critic feedback\n",
            "\n",
            "Reasoning Agent (X):\n",
            "  Purpose: Primary reasoning and answer generation\n",
            "  Features: Chain-of-thought reasoning, Confidence estimation, Step-by-step analysis\n",
            "  Z-Integration: Reasoning patterns evolve based on critique\n",
            "\n",
            "Tool Agent (Y):\n",
            "  Purpose: External tool integration\n",
            "  Features: Web search, Database queries, Fact checking, Calculations\n",
            "  Z-Integration: Tool selection optimized by performance history\n",
            "\n",
            "Critic Agent (Z):\n",
            "  Purpose: Self-assessment and continuous learning\n",
            "  Features: Quality assessment, Hallucination detection, Improvement suggestions\n",
            "  Z-Integration: Core of the self-learning mechanism\n",
            "\n",
            "üß† Z-Reasoning Learning Mechanism Analysis\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"729455bb-00f2-48d2-9d3f-ca3d06e3e871\" class=\"plotly-graph-div\" style=\"height:800px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"729455bb-00f2-48d2-9d3f-ca3d06e3e871\")) {                    Plotly.newPlot(                        \"729455bb-00f2-48d2-9d3f-ca3d06e3e871\",                        [{\"line\":{\"color\":\"blue\"},\"mode\":\"lines+markers\",\"name\":\"Overall Performance\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],\"y\":[0.6161231775748163,0.6028160524555028,0.5534142165240948,0.5286708259131919,0.5938093064293699,0.6573809413191046,0.6251298658992637,0.6617199042039092,0.6643208305165935,0.669857649934856,0.7729041667066548,0.6871064071503628,0.816470592987808,0.7316356174349896,0.6948368576792828,0.8558370026349185,0.8866529336558282,0.8658927426686364,0.9076706314553582,0.9162660123225618],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"line\":{\"color\":\"green\"},\"mode\":\"lines\",\"name\":\"Retrieval\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],\"y\":[0.39918326204491583,0.4412116227931967,0.4983289193725187,0.4714930453827624,0.5292592735542684,0.4966387271325572,0.5864120620392922,0.6327678153745342,0.6730976074219132,0.6565635137330257,0.6831203158787396,0.669871050896703,0.7096934500718984,0.7746056762161929,0.8171483915310659,0.7886106484192087,0.8389309781841657,0.861806979567201,0.860285478157849,0.8974127169203541],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"line\":{\"color\":\"orange\"},\"mode\":\"lines\",\"name\":\"Reasoning\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],\"y\":[0.4330466474877865,0.4279236262135764,0.4642865009274989,0.5078027699761651,0.5887854747258876,0.5407784196422931,0.6262340191036385,0.6457853540561381,0.5697576196671948,0.6497229409021199,0.7288765523912922,0.7149466908313749,0.6744168352140459,0.7210387913113566,0.7229734344505071,0.8556763827911154,0.8142729562130446,0.9114242370725858,0.7427740934665201,0.8808135690967989],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"marker\":{\"color\":\"lightcoral\"},\"name\":\"Success Rate\",\"x\":[\"Basic\",\"Semantic\",\"Hybrid\",\"Adaptive\"],\"y\":[0.6,0.7,0.8,0.85],\"type\":\"bar\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"labels\":[\"Interactions\",\"Strategies\",\"Patterns\",\"Feedback\"],\"name\":\"Memory\",\"values\":[150,25,40,80],\"type\":\"pie\",\"domain\":{\"x\":[0.55,1.0],\"y\":[0.0,0.375]}}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.625,1.0]},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0]},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.625,1.0]},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.0,0.45]},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.0,0.375]},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Learning Feedback Loop\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Performance Metrics Evolution\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Strategy Adaptation Process\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.375,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Memory Utilization\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.375,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"Z-Reasoning Learning Mechanism\"},\"height\":800},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('729455bb-00f2-48d2-9d3f-ca3d06e3e871');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìö Learning Principles:\n",
            "-------------------------\n",
            "\n",
            "Self-Consistency Checking:\n",
            "  ‚Ä¢ Agent compares outputs with retrieved context\n",
            "  ‚Ä¢ Implementation: Hallucination detection, factual grounding checks\n",
            "  ‚Ä¢ Benefit: Reduces false information generation\n",
            "\n",
            "Performance Reflection:\n",
            "  ‚Ä¢ Continuous assessment of reasoning quality\n",
            "  ‚Ä¢ Implementation: Critic agent evaluation, confidence scoring\n",
            "  ‚Ä¢ Benefit: Identifies weaknesses and improvement areas\n",
            "\n",
            "Strategy Evolution:\n",
            "  ‚Ä¢ Adaptive selection of retrieval and reasoning approaches\n",
            "  ‚Ä¢ Implementation: Performance-based strategy ranking and selection\n",
            "  ‚Ä¢ Benefit: Optimizes approach for different query types\n",
            "\n",
            "Memory Integration:\n",
            "  ‚Ä¢ Long-term storage and retrieval of past experiences\n",
            "  ‚Ä¢ Implementation: SQLite database with structured memory storage\n",
            "  ‚Ä¢ Benefit: Enables learning from past successes and failures\n",
            "üöÄ Initializing Self-Learning RAG Agent with Z-Reasoning\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1362197085.py:1261: LangChainDeprecationWarning:\n",
            "\n",
            "The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:\n",
            "\n",
            "\n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea54cb8b198840be8fca420c33821ffc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43ac23d5c9a54b3b978ff9c38b33bee9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "953a361a9565416b94c7a040fd89128a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "456abcb2b26f492c8fefa47cf3f860e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a8d59a6b2a24bdca4828689774e9f64"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96d4af31e7dc4c6385272aa21ad593ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f67c13fae3c04cfea27fc516bc25063e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2de7e4ba53a441bbb5951a8f57110013"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f9e688e8c2243d0b217a2c742f16105"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1ba6e43784944f5a3aa6b42b1236f99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94e44da422784514a23b20e3f5f414f8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Error during execution: Could not import chromadb python package. Please install it with `pip install chromadb`.\n"
          ]
        }
      ]
    }
  ]
}